{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Homework 6 - LearNNing Deeply\n",
    "### Your Name: Alyssa Kern\n",
    "\n",
    "**COLLABORATED WITH:_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "**Submit a PDF export of your notebook (100% PENALTY IF NO PDF IS SUBMITTED)** \n",
    "\n",
    "* File > Export Notebook to HTML (you cannot upload .html to Compass), open .html in browser, print to PDF\n",
    "    - **_Ensure your code is not cut off_**\n",
    "\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "- [Theory](#Theory)\n",
    "    - [[9 Points] Gradient Descent](#[9-Points]-Gradient-Descent)\n",
    "    - [Acitvation Functions](#Acitvation-Functions)\n",
    "        - [[5 Points] Part 1](#[5-Points]-Part-1)\n",
    "        - [[3 Points] Part 2](#[3-Points]-Part-2)\n",
    "        - [[3 Points] Part 3](#[3-Points]-Part-3)\n",
    "- [Neural Network Anatomy](#Neural-Network-Anatomy)\n",
    "    - [[2 Points] Create ReLU](#[2-Points]-Create-ReLU)\n",
    "    - [[8 Points] Create Base Objects](#[8-Points]-Create-Base-Objects)\n",
    "    - [[6 Points] Evaluate Hidden Layer $z$s](#[6-Points]-Evaluate-Hidden-Layer-$z$s)\n",
    "    - [[4 Points] Evaluate Output Layer $z$s](#[4-Points]-Evaluate-Output-Layer-$z$s)\n",
    "    - [[4 Points] Print $\\hat{y}_1$](#[4-Points]-Print-$\\hat{y}_1$)\n",
    "    - [[4 Points] Print $\\frac{\\partial C_1}{\\partial w_{1,3}^2}$](#[4-Points]-Print-$\\frac{\\partial-C_1}{\\partial-w_{1,3}^2}$)\n",
    "    - [[4 Points] Print $\\frac{\\partial C_1}{\\partial b_2^1}$](#[4-Points]-Print-$\\frac{\\partial-C_1}{\\partial-b_2^1}$)\n",
    "    - [[8 Points] The First Batch](#[8-Points]-The-First-Batch)\n",
    "- [Application](#Application)\n",
    "    - [Model 1](#Model-1)\n",
    "        - [[6 Points] Initialization](#[6-Points]-Initialization)\n",
    "        - [[3 Points] Compilation](#[3-Points]-Compilation)\n",
    "        - [[9 Points] Fitting](#[9-Points]-Fitting)\n",
    "        - [[2 Points] Evaluation](#[2-Points]-Evaluation)\n",
    "    - [Model 2](#Model-2)\n",
    "        - [[6 Points] Initialization](#[6-Points]-Initialization)\n",
    "        - [[3 Points] Compilation](#[3-Points]-Compilation)\n",
    "        - [[9 Points] Fitting](#[9-Points]-Fitting)\n",
    "        - [[2 Points] Evaluation](#[2-Points]-Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 40, 20, 20)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 9 + 5 + 3 + 3\n",
    "b = 2+8+6+4+4+4+4+8\n",
    "c = 6+3+9+2\n",
    "d = c\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "\n",
    "# Theory\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "## [9 Points] Gradient Descent \n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Compare and contrast the three different gradient descent algorithms we have covered with respect to:\n",
    "\n",
    "1. Whether or not they converge\n",
    "2. Their relative speed\n",
    "3. Whether they can escape non-global minima\n",
    "\n",
    "**Points**: 3 algorithms Ã— 3 characteristics = 9 points."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "The three gradient descent algorithms we have covered so far are:\n",
    "\n",
    "* Batch\n",
    "    - will converge as iteration goes to infinity\n",
    "    - very slow\n",
    "    - can get stuck at non-global minima\n",
    "\n",
    "* Stochastic \n",
    "    - will never converge\n",
    "    - fastest\n",
    "    - can 'jump' out of local minima\n",
    "\n",
    "* Mini-batch \n",
    "    - will never converge, but better than stochastic\n",
    "    - fast\n",
    "    - can escape local minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "## Acitvation Functions\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "*************\n",
    "### [5 Points] Part 1\n",
    "\n",
    "What is the relationship between the output layer activation function and $\\hat{y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**The parameters in a neural network are essentially input weights/biases that are fed through a series of activation functions. They first go through hidden layers until they finally go through the output layer, which will give us the value of $\\hat{y}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "### [3 Points] Part 2\n",
    "\n",
    "Which activation function discussed in lecture can lead to dead neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**The ReLU activation function can lead to dead neurons. This happens when the derivative is zero for all negative numbers.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "### [3 Points] Part 3\n",
    "\n",
    "Name an activation function based off of your answer above that was discussed in class that solves the dead neuron problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**An activation function that can solve the dead neuron problem is 'leaky ReLU'. The leaky ReLU modifies the function so that small negative values are allowed when the input is less than zero.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "# Neural Network Anatomy\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "**Directions:** The output should be from code chunks.\n",
    "You will likely find it helpful to first solve by hand before coding the solutions. \n",
    "*I genuinely always do this for more involved coding problems*.\n",
    "\n",
    "We are going to work with this neural network:\n",
    "\n",
    "![](hw_nn.png)\n",
    "\n",
    "Here is our made-up data:\n",
    "\n",
    "\n",
    "| Profit   | Step 1: Collect Pants | Step 2: ? |\n",
    "|----------|-----------|----------------|\n",
    "| -10      | 20        | -40            |\n",
    "| 1        | 5         | 42             |\n",
    "| $y_3$    | $x_{1,3}$ | $x_{2,3}$      |\n",
    "| $\\vdots$ | $\\vdots$  | $\\vdots$       |\n",
    "\n",
    "\n",
    "Here are some weights:\n",
    "\n",
    "| Layer  | Neuron | Input 1 Weight    | Input 2 Weight    | Input 3 Weight    | Bias        |\n",
    "|--------|--------|-------------------|-------------------|-------------------|-------------|\n",
    "| Hidden | Top    | $w^1_{1,1} = 0.3$ | $w^1_{1,2} = 0.4$ | *NA*              | $b^1_1 = 1$ |\n",
    "| Hidden | Middle | $w^1_{2,1} = 0.1$ | $w^1_{2,2} = 0.6$ | *NA*              | $b^1_2 = 1$ |\n",
    "| Hidden | Bottom | $w^1_{3,1} = 0.7$ | $w^1_{3,2} = 0.1$ | *NA*              | $b^1_3 = 1$ |\n",
    "| Output | Yup.   | $w^2_{1,1} = 0.9$ | $w^2_{1,2} = 0.7$ | $w^2_{1,3} = 0.5$ | $b^2_1 = 1$ |\n",
    "\n",
    "\n",
    "To make things a bit easier for you, use ReLU for the **hidden layer** activation function:\n",
    "\n",
    "$$\\text{ReLU}(z) = \\begin{cases}\n",
    "0 & \\text{if } z \\leq 0\\\\\n",
    "z & \\text{if } z > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "Use the swanky `None` function for the **output layer** activation function:\n",
    "\n",
    "$$\\text{none}(z) = z$$\n",
    "\n",
    "\n",
    "Use the MSE for the cost function:\n",
    "\n",
    "$$ C_k = (\\hat{y}_k - y_k)^2$$\n",
    "\n",
    "And let\n",
    "\n",
    "$$\\eta = 0.05$$\n",
    "\n",
    "***********************\n",
    "\n",
    "**OBJECTIVE:** Determine how much we will adjust $w^2_{1,3}$ and $b^1_2$ using a batch of two observations.\n",
    "You will need to calculate the first iteration's $\\frac{\\partial C_1}{\\partial w^2_{1,3}}$ and $\\frac{\\partial C_1}{\\partial b^1_2}$.\n",
    "I will give you the second iteration's $\\frac{\\partial C_2}{\\partial w^2_{1,3}}$ and $\\frac{\\partial C_2}{\\partial b^1_2}$.\n",
    "\n",
    "***********\n",
    "\n",
    "## [2 Points] Create ReLU\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Create a function `r()` for ReLU and `r_prime` for its derivative.\n",
    "\n",
    "*Note: you do not need any packages to do this sections.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def r(z):\n",
    " #   if z <= 0:\n",
    "  #      return(0)\n",
    "   # if z > 0:\n",
    "    #    return(z)\n",
    "#def r_prime(z):\n",
    " #   if z <= 0:\n",
    "  #      return(0)\n",
    "   # if z > 0:\n",
    "    #    return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a simple mistake on my part for not being careful with the definition and interpretation of the relu function. I misinterpreted the task and assumed that if z was greater than 0 and was a constant, that the derivative would be 0. However, the relu function is a piecewise linear function. The derivative of relu is simply the slope where the input is positive. Since relu is linear, the slope is constant and equal to 1. This is why for r_prime(z) should return 1 instead of 0 if z is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(z):\n",
    "    if z <= 0:\n",
    "        return(0)\n",
    "    if z > 0:\n",
    "        return(z)\n",
    "def r_prime(z):\n",
    "    if z <= 0:\n",
    "        return(0)\n",
    "    if z > 0:\n",
    "        return(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "## [8 Points] Create Base Objects\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "Create unique objects in `python` for the $y_1$, $x_{1,1}$, $x_2,1$, the respective weights and biases, and $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = -10\n",
    "x111 = 20\n",
    "x112 = -40\n",
    "w111 = 0.3\n",
    "w112 = 0.4\n",
    "w121 = 0.1\n",
    "w122 = 0.6\n",
    "w131 = 0.7\n",
    "w132 = 0.1\n",
    "w211 = 0.9\n",
    "w212 = 0.7\n",
    "w213 = 0.5\n",
    "bias = 1\n",
    "learn_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "## [6 Points] Evaluate Hidden Layer $z$s\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Create unique objects for the hidden layer's $z$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z11 = w111 * x111 + w112 * x112 + bias\n",
    "z12 = w121 * x111 + w122 * x112 + bias\n",
    "z13 = w131 * x111 + w132 * x112 + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "## [4 Points] Evaluate Output Layer $z$s\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Using the defined values of $z$ above, calculate $z_1^2$ using your definied function(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z21 = w211 * r(z11) + w212 * r(z12) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simply forgot to add the third component of the ouput layer, 'w213 * r(z13)'. The output layer's inputs are made up from the layers before it, as shown in the hidden layer z's section above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z21 = w211 * r(z11) + w212 * r(z12) + w213 * r(z13) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "## [4 Points] Print $\\hat{y}_1$\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "Calculate and print $\\hat{y}_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5\n"
     ]
    }
   ],
   "source": [
    "y1hat = r(z21)\n",
    "print(y1hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "## [4 Points] Print $\\frac{\\partial C_1}{\\partial w_{1,3}^2}$\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "Using the chain rule, evaluate and print $\\frac{\\partial C_1}{\\partial w_{1,3}^2}$\n",
    "\n",
    "**2 POINTS EXTRA CREDIT** \n",
    "\n",
    "- Line 1 -  Use LaTeX to write out the chain rule as we did in class (the partial derivatives).\n",
    "- Line 2 - Plug in the to the derivatives the gerenerlized evaluation (no values, i.e. use $w^2_{1,3}$) of those partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dc_dw = 2 * (r(z21) - y1) * r(z21) * (1 - r(z21))\n",
    "#print(dc_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for Ck was given to us previously in the homework and C1 is equal to (y1hat - y1) ^ 2. Using the chain rule, we bring down the 2 and multiply it by (y1hat - y1) ^ 1. Then we must multiply that by the derivative of (y1hat - y1), which is 1 * r(z13). I mistakenly used r(z21) instead of y1hat and used r(z21) instead of r(z13). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_dw = ( 2 * (y1hat - y1)) * 1 * r(z13)\n",
    "dc_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA CREDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial C_1}{\\partial w_{1,3}^2} $$\n",
    "$$ = \\frac{\\partial C_1}{\\partial X} * \\frac{\\partial z_{1}^2}{\\partial z_{1}^2}*\\frac{\\partial z_{1}^2(w_{1,3}^2)}{\\partial w_{1,3}^2} $$\n",
    "$$  = [2(\\hat{y}_1-y_1)] * 1 * ReLU(z_{3}^1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "##  [4 Points] Print $\\frac{\\partial C_1}{\\partial b_2^1}$\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "Using the chain rule, evaluate and print $\\frac{\\partial C_1}{\\partial b_2^1}$.\n",
    "\n",
    "**2 POINTS EXTRA CREDIT** \n",
    "\n",
    "- Line 1 -  Use LaTeX to write out the chain rule as we did in class (the partial derivatives).\n",
    "- Line 2 - Plug in the to the derivatives the gerenerlized evaluation (no values, i.e. use $w^2_{1,3}$) of those partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dc_db = 2 * (r(z21) - y1) * r(z21) * (1 - r(z21)) * w212 * r(z12) * (1 - r(z12))\n",
    "#print(dc_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, following the chain rule we bring down the 2 and multiply that by (y1hat - y1) ^ 1. Then we multiply that by the derivative of the inside which is 1 * w212 * r_prime(z12) , not r(z21) * (1 - r(z21)) * w212 * r(z12) * (1 - r(z12)) like I put. I also made the same mistake of using r(z21) as y1hat again in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_db = ( 2 *(y1hat - y1)) * 1 * w212 * r_prime(z12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA CREDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial C_1}{\\partial b_{2}^1} $$\n",
    "$$ = \\frac{\\partial C_1}{\\partial none} * \\frac{\\partial none(z_{1}^2)}{\\partial z_{1}^2} * \\frac{\\partial z_{1}^2(ReLU_{2}^1)}{\\partial ReLU_{2}^1} * \\frac{\\partial ReLU_{2}^1}{\\partial z_{2}^1} *\n",
    "\\frac{\\partial z_{2}^1}{\\partial b_{2}^1} $$\n",
    "$$ = [2(\\hat{y}_1-y_1)] * 1 * w_{1,2}^2 * ReLU_{2}^1\\prime{z_{2}^1)} * 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "## [8 Points] The First Batch\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "If your code is correct, you can verify $\\frac{\\partial C_2}{\\partial w^2_{1,3}} = 703.134$ exactly and \n",
    "$\\frac{\\partial C_2}{\\partial b^1_2} = 56.574$ exactly.\n",
    "\n",
    "To wrap up this problem, by how much do we adjust $w^2_{1,3}$ and $b^1_2$ by respectively after our first tiny batch (not their new values)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was initially unsure of how to verify that the derivative of C2 in respect to w213 was equal to 703.134 and the derivative of C2 in respect to b12 was 56.574. To do this, I had to multiply the negative learn rate (-0.05) by the value I got for dc_dw + the given value for it divided by 2. I did the same process again with dc_db. This confirms that our code is correct and verifies these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.41435"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w^2_{1,3} changes by:\n",
    "-learn_rate * (dc_dw + 703.134)/2\n",
    "\n",
    "# b^2_1 changes by:\n",
    "-learn_rate * (dc_db + 56.574)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "# Application\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "**10% EXTRA CREDIT OPPORTUNITY**\n",
    "\n",
    "In the application section, you will fit two models to predict `l_incwage`:\n",
    "\n",
    "1. A single layer neural network\n",
    "2. Any neural network of your choosing using the Keras package\n",
    "\n",
    "The 10% extra credit on this assignment is awarded to the student with the chosen model that has the lowest testing RMSE.\n",
    "There is a new folder `AAA HW6 Competition` in the Box folder where you uploaded your final project data, in which you will submit your evaluation RMSE as a box note.\n",
    "I have provided an example (the RMSE of the first model) in the folder: copy and paste the entire RMSE into the title on the Box note followed by your name.\n",
    "\n",
    "**CONDITIONS FOR WINNING**\n",
    "\n",
    "1. The student with the best RMSE must provide a separate Jupyter Notebook to me containing the winning model after the competition closes. This notebook will be shared with the entire class (anonymously, if you prefer). The competition closes immediately after the HW due date.\n",
    "2. I must be able to `Restart Kernel and Run All Cells...` to produce the winning evaluation RMSE.\n",
    "3. You must use the setup (`train_test_split()`, etc.) provided to you below.\n",
    "4. You are limited to using Keras package exclusively for the neural network itself. However, you can use any architecture, number of layers, optimizer, learning rate, loss, etc. in your final model.\n",
    "5. You need not use all of the features from the HW data, but you may neither add additional features to the data nor transform any of the variables.\n",
    "6. You may have multiple submissions. If you submit more than one, you must delete your original submission and then add a new one to produce an updated time stamp to your submission.\n",
    "7. You must submit the RMSE in the created Box folder following the format I have provided.\n",
    "8. In the extremely unlikely event, I reserve the right to void any submission.\n",
    "\n",
    "Additional comments:\n",
    "\n",
    "- If there is a tie, the winner is chosen by the earliest submission. Submissions are time stamped.\n",
    "- You do not need to compete, however, you still need to fit a second model that does not match the first for this HW.\n",
    "- If you have any clarifying questions, please email me\n",
    "\n",
    "\n",
    "\n",
    "****************\n",
    "You will need our `stdz()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import pandas as pd\n",
    "\n",
    "# processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# algorithms\n",
    "from tensorflow import keras\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(490)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(490)\n",
    "tf.random.set_seed(490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hw_data.csv')\n",
    "df.set_index(['county', 'ind1990', 'occ2010'], inplace = True)\n",
    "\n",
    "# Pandas is doing the weird \"I don't want to join() properly\"\n",
    "# so I am going to force it with pd.concat()\n",
    "df = pd.concat([\n",
    "    df.drop(columns = 'degree'),\n",
    "    pd.get_dummies(df['degree'], drop_first = True)\n",
    "], axis = 1)\n",
    "\n",
    "y = df['l_incwage']\n",
    "x = df.drop(columns = 'l_incwage')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                   train_size = 2/3,\n",
    "                                                   random_state = 490)\n",
    "\n",
    "x_train = x_train.apply(stdz, axis = 0)\n",
    "x_test  = x_test.apply(stdz, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "## Model 1\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Remember that this model should only have one hidden layer.\n",
    "\n",
    "*********\n",
    "### [6 Points] Initialization\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "**[4 Points]** Define a \n",
    "\n",
    "1. Sequential model\n",
    "2. with an appropriate input shape\n",
    "3. with one hidden layer that has a size of 100 and a ReLU activation function\n",
    "4. with an output layer that has a shape of 1 and a linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(shape = x_train.shape[1]))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'linear' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 Points]** Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               1500      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,601\n",
      "Trainable params: 1,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "### [3 Points] Compilation\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Compile the model with \n",
    "\n",
    "1. a loss of `'mean_squared_error'`\n",
    "2. a metric of `tf.keras.metrics.RootMeanSquaredError()`\n",
    "3. an `'adam'` optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',\n",
    "             metrics = tf.keras.metrics.RootMeanSquaredError(),\n",
    "             optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### [9 Points] Fitting\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "1. Set up early stopping with a patience of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with\n",
    "\n",
    "2. a batch size of 128\n",
    "3. 20 epochs\n",
    "4. a validation split of 1/5\n",
    "5. input the early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3275/3275 [==============================] - 4s 1ms/step - loss: 13.2143 - root_mean_squared_error: 3.2100 - val_loss: 0.5843 - val_root_mean_squared_error: 0.7644\n",
      "Epoch 2/20\n",
      "3275/3275 [==============================] - 3s 921us/step - loss: 0.5690 - root_mean_squared_error: 0.7543 - val_loss: 0.5649 - val_root_mean_squared_error: 0.7516\n",
      "Epoch 3/20\n",
      "3275/3275 [==============================] - 3s 883us/step - loss: 0.5552 - root_mean_squared_error: 0.7451 - val_loss: 0.5577 - val_root_mean_squared_error: 0.7468\n",
      "Epoch 4/20\n",
      "3275/3275 [==============================] - 3s 906us/step - loss: 0.5493 - root_mean_squared_error: 0.7411 - val_loss: 0.5525 - val_root_mean_squared_error: 0.7433\n",
      "Epoch 5/20\n",
      "3275/3275 [==============================] - 3s 891us/step - loss: 0.5510 - root_mean_squared_error: 0.7423 - val_loss: 0.5521 - val_root_mean_squared_error: 0.7430\n",
      "Epoch 6/20\n",
      "3275/3275 [==============================] - 3s 866us/step - loss: 0.5462 - root_mean_squared_error: 0.7391 - val_loss: 0.5541 - val_root_mean_squared_error: 0.7443\n",
      "Epoch 7/20\n",
      "3275/3275 [==============================] - 3s 850us/step - loss: 0.5453 - root_mean_squared_error: 0.7385 - val_loss: 0.5516 - val_root_mean_squared_error: 0.7427\n",
      "Epoch 8/20\n",
      "3275/3275 [==============================] - 3s 923us/step - loss: 0.5434 - root_mean_squared_error: 0.7372 - val_loss: 0.5491 - val_root_mean_squared_error: 0.7410\n",
      "Epoch 9/20\n",
      "3275/3275 [==============================] - 3s 922us/step - loss: 0.5441 - root_mean_squared_error: 0.7376 - val_loss: 0.5536 - val_root_mean_squared_error: 0.7440ot_mean_squared\n",
      "Epoch 10/20\n",
      "3275/3275 [==============================] - 3s 867us/step - loss: 0.5430 - root_mean_squared_error: 0.7369 - val_loss: 0.5501 - val_root_mean_squared_error: 0.7417\n",
      "Epoch 11/20\n",
      "3275/3275 [==============================] - 3s 909us/step - loss: 0.5414 - root_mean_squared_error: 0.7358 - val_loss: 0.5471 - val_root_mean_squared_error: 0.7397\n",
      "Epoch 12/20\n",
      "3275/3275 [==============================] - 3s 1ms/step - loss: 0.5410 - root_mean_squared_error: 0.7355 - val_loss: 0.5511 - val_root_mean_squared_error: 0.7424 1s - loss: 0.5380 - root_mean_squared_e - ETA: 1s - loss: 0.5397 - root_mea\n",
      "Epoch 13/20\n",
      "3275/3275 [==============================] - 3s 941us/step - loss: 0.5442 - root_mean_squared_error: 0.7377 - val_loss: 0.5480 - val_root_mean_squared_error: 0.7403\n",
      "Epoch 14/20\n",
      "3275/3275 [==============================] - 3s 913us/step - loss: 0.5429 - root_mean_squared_error: 0.7368 - val_loss: 0.5489 - val_root_mean_squared_error: 0.7408_mean_squ\n",
      "Epoch 15/20\n",
      "3275/3275 [==============================] - 3s 841us/step - loss: 0.5433 - root_mean_squared_error: 0.7371 - val_loss: 0.5502 - val_root_mean_squared_error: 0.7418\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                   batch_size = 128,\n",
    "                   epochs = 20,\n",
    "                   validation_split = 1/5,\n",
    "                   callbacks = [es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a plot of the history.\n",
    "\n",
    "6. plot only the training RSMSE \n",
    "7. and validation RMSE\n",
    "8. adjust the ylimits\n",
    "9. Add a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.593672</td>\n",
       "      <td>1.895698</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.764375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.752570</td>\n",
       "      <td>0.564869</td>\n",
       "      <td>0.751578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.555093</td>\n",
       "      <td>0.745046</td>\n",
       "      <td>0.557659</td>\n",
       "      <td>0.746766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.550452</td>\n",
       "      <td>0.741925</td>\n",
       "      <td>0.552526</td>\n",
       "      <td>0.743321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.548124</td>\n",
       "      <td>0.740354</td>\n",
       "      <td>0.552108</td>\n",
       "      <td>0.743040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  root_mean_squared_error  val_loss  val_root_mean_squared_error\n",
       "0  3.593672                 1.895698  0.584270                     0.764375\n",
       "1  0.566362                 0.752570  0.564869                     0.751578\n",
       "2  0.555093                 0.745046  0.557659                     0.746766\n",
       "3  0.550452                 0.741925  0.552526                     0.743321\n",
       "4  0.548124                 0.740354  0.552108                     0.743040"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n",
    "train_results = pd.DataFrame(history.history)\n",
    "train_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " #train_results.plot()\n",
    "\n",
    "\n",
    "#plt.ylim(0, 1)\n",
    "#plt.grid(True)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot I initially provided did not plot only the training RMSE or validation RMSE. I accidently put the train_results = pd.DataFrame(history.history) portion in the previous section, which was not useful. I also neglected to explicitly state that the train_results plot needed to show the root mean squared error and the val root mean squared error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = pd.DataFrame(history.history)\n",
    "\n",
    "train_results[['root_mean_squared_error',\n",
    "               'val_root_mean_squared_error']].plot()\n",
    "\n",
    "plt.ylim(0.7, 0.8)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "### [2 Points] Evaluation\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "1. Evaluate the model\n",
    "2. Only print the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8187/8187 [==============================] - 4s 505us/step - loss: 0.5516 - root_mean_squared_error: 0.7427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7427072525024414"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf = model.evaluate(x_test, y_test)\n",
    "acc_nn = model_perf[1]\n",
    "acc_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "## Model 2\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "Your second chosen model may not match the model above. \n",
    "Have fun!\n",
    "\n",
    "*********\n",
    "### [6 Points] Initialization\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Define your chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.Input(shape = x_train.shape[1]))\n",
    "model2.add(keras.layers.Dense(300, activation = 'relu'))\n",
    "model2.add(keras.layers.Dense(200, activation = LeakyReLU(alpha = 0.1)))\n",
    "\n",
    "\n",
    "\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Input(shape = x_train.shape[1]))\n",
    "#model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "#model.add(keras.layers.Dense(1, activation = 'linear' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "### [3 Points] Compilation\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "Compile your model with your chosen\n",
    "\n",
    "1. loss\n",
    "3. optimizer\n",
    "\n",
    "You must use at least the RMSE metric:\n",
    "\n",
    "3. a metric of `tf.keras.metrics.RootMeanSquaredError()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'mean_absolute_percentage_error',\n",
    "             metrics = tf.keras.metrics.RootMeanSquaredError(),\n",
    "             optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### [9 Points] Fitting\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "- **[5 Points]** Fit your chosen model\n",
    "- **[4 Points]** Plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es = keras.callbacks.EarlyStopping(patience = 3)\n",
    "\n",
    "#history.history\n",
    "#train_results_2 = pd.DataFrame(history.history)\n",
    "#train_results_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I made a very silly mistake here by not even fitting my model. The code I originally submitted only printed out the head of the model, but did not fit it. I also failed to include the code that was nnecessary to show the history plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "13098/13098 [==============================] - 18s 1ms/step - loss: 8.8433 - root_mean_squared_error: 1.5247 - val_loss: 5.6305 - val_root_mean_squared_error: 0.7700\n",
      "Epoch 2/30\n",
      "13098/13098 [==============================] - 19s 1ms/step - loss: 5.6423 - root_mean_squared_error: 0.7717 - val_loss: 5.5522 - val_root_mean_squared_error: 0.7583\n",
      "Epoch 3/30\n",
      "13098/13098 [==============================] - 19s 1ms/step - loss: 5.5173 - root_mean_squared_error: 0.7585 - val_loss: 5.6229 - val_root_mean_squared_error: 0.7641\n",
      "Epoch 4/30\n",
      "13098/13098 [==============================] - 19s 1ms/step - loss: 5.4711 - root_mean_squared_error: 0.7533 - val_loss: 5.4855 - val_root_mean_squared_error: 0.7544\n",
      "Epoch 5/30\n",
      "13098/13098 [==============================] - 19s 1ms/step - loss: 5.4630 - root_mean_squared_error: 0.7532 - val_loss: 5.4872 - val_root_mean_squared_error: 0.7575: 1s - loss: 5\n",
      "Epoch 6/30\n",
      "13098/13098 [==============================] - 19s 1ms/step - loss: 5.4233 - root_mean_squared_error: 0.7492 - val_loss: 5.4167 - val_root_mean_squared_error: 0.7473\n",
      "Epoch 7/30\n",
      "13098/13098 [==============================] - 20s 2ms/step - loss: 5.4121 - root_mean_squared_error: 0.7475 - val_loss: 5.4696 - val_root_mean_squared_error: 0.7557: 1s - loss: 5.4120 - root_mean_s\n",
      "Epoch 8/30\n",
      "13098/13098 [==============================] - 23s 2ms/step - loss: 5.4060 - root_mean_squared_error: 0.7459 - val_loss: 5.4178 - val_root_mean_squared_error: 0.7497\n",
      "Epoch 9/30\n",
      "13098/13098 [==============================] - 19s 1ms/step - loss: 5.3932 - root_mean_squared_error: 0.7455 - val_loss: 5.4273 - val_root_mean_squared_error: 0.7511\n"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience = 3)\n",
    "history2 = model2.fit(x_train, y_train,\n",
    "                     batch_size = 32,\n",
    "                     epochs = 30,\n",
    "                     validation_split = 0.2,\n",
    "                     callbacks = [es])\n",
    "\n",
    "train_results_2 = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "### [2 Points] Evaluation\n",
    "[TOP](#Homework-6---LearNNing-Deeply)\n",
    "\n",
    "\n",
    "1. Evaluate the model\n",
    "2. Only print the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8187/8187 [==============================] - 6s 776us/step - loss: 5.4249 - root_mean_squared_error: 0.7509 0s - loss: 5.4207 - root_mean_squared_e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7508773803710938"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf_2 = model2.evaluate(x_test, y_test)\n",
    "acc_nn_2 = model_perf_2[1]\n",
    "acc_nn_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
