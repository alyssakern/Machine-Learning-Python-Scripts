{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - EDA, Regression, & You\n",
    "### Your Name: Alyssa Kern\n",
    "\n",
    "**COLLABORATED WITH:_ _ _Randy Tang _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "**Submit a PDF export of your notebook (100% PENALTY IF NO PDF IS SUBMITTED)** \n",
    "\n",
    "* Option 1 - File > Export Notebook to PDF\n",
    "* Option 2 - File > Export Notebook to HTML (you cannot upload .html to Compass), open .html in browser, print to PDF\n",
    "    - You need to use this option if you have math with `\\begin{align} ... \\end{align}`.\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [Intuition](#Intuition)\n",
    "    - [[5 Points] HoML Ch1 Q12](#[5-Points]-HoML-Ch1-Q12)\n",
    "    - [[5 Points] HoML Ch1 Q17](#[5-Points]-HoML-Ch1-Q17)\n",
    "* [Theory](#Theory)\n",
    "    - [[20 Points] Bias-Variance Tradeoff](#[20-Points]-Bias-Variance-Tradeoff)\n",
    "    - [[15 Points] Unbiased SLR Estimator](#[15-Points]-Unbiased-SLR-Estimator)\n",
    "* [Application - EDA](#Application---EDA)\n",
    "    - [[10 Points] Setup](#[10-Points]-Setup)\n",
    "    - [[5 Points] Summary Statistics](#[5-Points]-Summary-Statistics)\n",
    "    - [[10 Points] Figures](#[10-Points]-Figures)\n",
    "- [Application - Regression](#Application---Regression)\n",
    "    - [[10 Points] Data Prep](#[10-Points]-Data-Prep)\n",
    "    - [[2 Points] Null Model](#[2-Points]-Null-Model)\n",
    "    - [[4 Points] Backward Selection](#[4-Points]-Backward-Selection)\n",
    "    - [[10 Points] Regularization](#[10-Points]-Regularization)\n",
    "    - [[4 Points] Comparison](#[4-Points]-Comparison)\n",
    "\n",
    "\n",
    "\n",
    "***********************************************************************************************\n",
    "# Intuition\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "**Points are not awarded for correct answers**. \n",
    "Instead, points are awarded for explainations of why you are correct or why you are wrong.\n",
    "You must attempt to answer the question to receive any points.\n",
    "Points for intuition questions are pass/fail.\n",
    "\n",
    "Answers such as \"I am correct/wrong\" or \"because the textbook says so\" are invalid. \n",
    "Show us you know what you are talking about: explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 Points] HoML Ch1 Q12\n",
    "\n",
    "What is the difference between a model parameter and a learning algorithm's hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model parameter is a variable within a model that has a value that can be estimated, usually through historical training data. These are also called fitted parameters. A learning algorithm's hyperparameter is a parameter that is adjustable and must be 'tuned' for a model to perform optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are you correct or wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 13 of our textbook, specifically page 396, it states that predicted values can be computed by the estimated moodel parameters. This defines the function/role of model parameters. In Chapter 13 page 400, the textbook says that many models have parameters that can be tuned (among other techniques) to avoid over fitting the training data. This description is defining hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 Points] HoML Ch1 Q17\n",
    "\n",
    "What is the purpose of the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set serves the purpose of providing an unbiased evaluation of the model's fit on the training data set while tuning the model hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are you correct or wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same reason I provided in the previous question. Chapter 13 in the textbook (page 400) states that cross-validation is one of the techniques that can be used for parameter tuning to avoid overfitting to the training data. It works by 'splitting' the training data set to simulate an out of sample prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Theory\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "You firstly get to show the bias-variance tradeoff.\n",
    "This is an excellent interview question, so you should know it.\n",
    "The second is that the conditional expectation of the OLS estimator is the population coefficient.\n",
    "\n",
    "**ENSURE YOUR MATH IS NOT CUT OFF IN YOUR FINAL PDF.** You cannot get points for math **_or_** explanations we cannot see.\n",
    "\n",
    "## [20 Points] Bias-Variance Tradeoff\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "Start from $E[e_i^2]$ and explain what you are doing each step.\n",
    "Points are awarded for the explanation, not the math.\n",
    "If something is zero, tell me why, even if you think it is obvious.\n",
    "Show me you know what you are doing.\n",
    "You are welcome to drop subscripts and $(\\cdot)$.\n",
    "\n",
    "To be specific, show that\n",
    "$$\n",
    "\\begin{align}\n",
    "    E[e_i^2] & = Bias[\\hat{f}(x_i)]^2 + V[\\hat{f}(x_i)] + V[\\epsilon_i]\\\\\n",
    "    & = (f(x_i) - E[\\hat{f}(x_i))^2 + E[(\\hat{f}(x_i) - E[\\hat{f}(x_i)])^2] + E[(\\epsilon - E[\\epsilon])^2]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ingredients:\n",
    "\n",
    "1. $y_i = \\hat{f}(x_i) + e_i$\n",
    "2. $y_i = f(x_i) + \\epsilon_i$\n",
    "3. $E[f(x_i)] = f(x_i)$\n",
    "4. $E[E[\\hat{f}(x_i)]] = E[\\hat{f}(x_i)] \\neq \\hat{f}(x_i)$\n",
    "5. $E[\\epsilon_i] = 0$\n",
    "6. $V[\\hat{f}(x_i)] = E[(\\hat{f}(x_i) - E[\\hat{f}(x_i)])^2] = E[\\hat{f}(x_i)^2] - E[\\hat{f}(x_i)]^2]$\n",
    "7. $Cov(\\hat{f}(x_i), \\epsilon_i) = E[(\\hat{f}(x_i) - E[\\hat{f}(x_i)])(\\epsilon_i - E[\\epsilon_i])] = 0$ by independence of $\\hat{f}$ and $\\epsilon$\n",
    "8. $(a + b + c) = a^2 + b^2 + c^2 + 2ab + 2ac + 2bc$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "Complete the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    E[e_i^2] & = E[(y - \\hat{f})^2] \\tag{by (1)}\\\\\n",
    "    & = E\\{[(f - E[\\hat{f}]) + \\epsilon + (E[\\hat{f}] - \\hat{f})]^2\\} \\tag{added zero}\\\\\n",
    "    & = E[(f-E[\\hat{f}])^2] + E[\\epsilon^2] + E[(E[\\hat{f}] - \\hat{f})^2] +\\\\\n",
    "    & + 2 E[(f - E[\\hat{f}])\\epsilon] + 2E[(f-E[\\hat{f}])(E[\\hat{f}]-\\hat{f})] + 2E[\\epsilon(E[\\hat{f}]-\\hat{f})] \\tag{by (8)} \\\\\n",
    "    & \\vdots\\\\    \n",
    "    & = Bias[\\hat{f}]^2 + V[\\epsilon] + V[\\hat{f}]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    E[e_i^2] & = E[(y - \\hat{f})^2] \\tag{by (1)}\\\\\n",
    "    & = E\\{[(f - E[\\hat{f}]) + \\epsilon + (E[\\hat{f}] - \\hat{f})]^2\\} \\tag{added zero}\\\\\n",
    "    & = E[(f-E[\\hat{f}])^2] + E[\\epsilon^2] + E[(E[\\hat{f}] - \\hat{f})^2] + \\\\\n",
    "    & 2 E[(f - E[\\hat{f}])\\epsilon] + 2E[(f-E[\\hat{f}])(E[\\hat{f}]-\\hat{f})] + \\\\\n",
    "    & 2E[\\epsilon(E[\\hat{f}]-\\hat{f})] \\tag{by (8)} \\\\  \n",
    "    & = (f-E[\\hat{f}])^2 + E[\\epsilon^2] + E[(E[\\hat{f}] - \\hat{f})^2] + \\\\\n",
    "    & 2(f - E[\\hat{f}])E[\\epsilon] + 2E[(f-E[\\hat{f}])(E[\\hat{f}]-\\hat{f})] + \\\\\n",
    "    & 2(E[\\epsilon])E[(E[\\hat{f}]-\\hat{f})] \\tag{by (3), simplify} \\\\  \n",
    "    & = (f-E[\\hat{f}])^2 + E[\\epsilon^2] + E[(E[\\hat{f}] - \\hat{f})^2] + \\\\\n",
    "    & 2E[(f-E[\\hat{f}])(E[\\hat{f}]-\\hat{f})] \\tag{by (5)} \\\\ \n",
    "    & = (f-E[\\hat{f}])^2 + E[\\epsilon^2] + E[(E[\\hat{f}] - \\hat{f})^2]  \\tag{by (7)} \\\\ \n",
    "    & = Bias[\\hat{f}]^2 + V[\\epsilon] + V[\\hat{f}] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second line uses the FOIL method to expand the expected value function, and lines 3, 4, 5, 6, 7, and 8 simplifies this action. We take the expected value of each element and take out any constant terms we can for easier evaluation. Line 9 constructs the terms in a way that can be simplified into what we want to find ( for example, expected value of epsilon squared will be the variance of epsilon).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 Points] Unbiased SLR Estimator\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "Show $E[\\hat{\\beta}|X] = \\beta$.\n",
    "\n",
    "*An aside on conditional expectation:* $E[\\hat{f}(x)] \\neq \\hat{f}(x)$*, however,* $E[\\hat{f}(x)|x] = \\hat{f}(x)$ *because* $\\hat{f}(x)$ *is a function of* $x$.\n",
    "\n",
    "Ingredients\n",
    "\n",
    "1. $y_i = \\alpha + x_i\\beta  + \\epsilon_i$\n",
    "2. $\\bar{y} = \\alpha + \\bar{x}\\beta$\n",
    "3. $E[h(x)|x] = h(x)$ for any function $h(x)$\n",
    "4. $E[\\epsilon|X] = 0$\n",
    "5. $\\hat{\\beta} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Complete the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\hat{\\beta} & = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (5)}\\\\\n",
    "  & \\vdots \\\\\n",
    "  & = \\beta  \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "  \\hat{\\beta} & = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (5)}\\\\\n",
    "  & E[\\hat{\\beta}|X] = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})E[(y_i - \\bar{y})|X]}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (3)} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(E[y_i|X] - E[\\bar{y}|X])}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{Using linearity of expectation} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(E[(\\alpha+x_i\\beta+\\epsilon_i)] - E[\\bar{y}|X])}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (1)} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(E[\\alpha+x_i\\beta+\\epsilon_i] - E[\\alpha+\\bar{x}\\beta])}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (2)} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(E[\\alpha+x_i\\beta] + E[\\epsilon_i] - E[\\alpha+\\bar{x}\\beta])}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{Simplify} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(E[\\alpha+x_i\\beta] - E[\\alpha+\\bar{x}\\beta])}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (4)} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})((\\alpha+x_i\\beta) - (\\alpha+\\bar{x}\\beta))}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{by (3)} \\\\\n",
    "&  = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(\\beta(x_i-\\bar{x}))}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\tag{Simplify} \\\\\n",
    "&  = \\beta(\\frac{\\sum_{i=1}^n(x_i - \\bar{x})^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}) \\tag{Simplify} \\\\\n",
    "&  = \\beta  \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second line re writes beta hat as the expected value of beta hat and the yi - y bar as the conditional expected value of itself given x. This allows us to break this term down into multiple components in line 3 (expected value of yi given x and expected value of y bar given x). Line 4 re writes the expected value of yi given x as the equation it represents (alpha plus xi * beta plus epsilon). Line 5 rewrites the expected value of y bar given x as alpha plus x bar * beta. These operations and substitutions will help us cancel values. Line 6 seperates the expected value of yi given x equation into two seperate expected values, this is because we know the expected value of epsilon is zero due to Gauss Markov assumptions. In line 8, we know that the expected values of the two terms in the numerator are themselves. Since they share a common factor, beta, we combine the two componetns as one and multiply by beta. Since beta is a constant, we can take it out of the summation. Now we have a fraction of two identical values, which turns into the value one and leaves beta left.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "# Application - EDA\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "*************\n",
    "## [10 Points] Setup\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** Set seaborn runtine configurations (title size, axes label size, xtick label size, ytick label size, figure size) larger than default (it need not match what is used in lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'axes.titlesize': 30,\n",
    "             'axes.labelsize': 25,\n",
    "             'xtick.labelsize': 16,\n",
    "             'ytick.labelsize': 16,\n",
    "             'figure.figsize': (10, 5)})\n",
    "sns.set_style(\"white\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point]** Import the homework data using whatever name for the object you like for the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id  county  occ2010  ind1990  l_incwage  full_time  \\\n",
      "0             601    1097     3600      831   9.862666          0   \n",
      "1             602    1097     3640      820   9.857444          1   \n",
      "2             801    1097     7810      641  10.203592          1   \n",
      "3             802    1097     4720      641   9.798127          0   \n",
      "4            2301    1097     6355       60  11.289782          1   \n",
      "...           ...     ...      ...      ...        ...        ...   \n",
      "785867  137743201   55117     5140       60   8.517193          0   \n",
      "785868  137743202   55117     8740      882  11.156251          1   \n",
      "785869  137743203   55117     4110      641   7.600902          0   \n",
      "785870  137753101   55117      205       11   9.104980          1   \n",
      "785871  137756301   55117     3410      642  10.239960          1   \n",
      "\n",
      "              degree  l_density  experience  puma_urate     occ_ba  \\\n",
      "0       some college   7.695985           6    5.917822   8.586593   \n",
      "1       some college   7.695985           5    5.917822   9.645899   \n",
      "2       some college   7.758590           7    5.454464   3.977090   \n",
      "3         hs or less   7.758590           7    5.454464   6.995643   \n",
      "4         hs or less   5.786897          18    3.103390   7.152717   \n",
      "...              ...        ...         ...         ...        ...   \n",
      "785867    hs or less   6.595917          34    1.312605  23.761010   \n",
      "785868    hs or less   6.595917          30    1.312605  16.031447   \n",
      "785869    hs or less   6.595917           0    1.312605  11.182924   \n",
      "785870  some college   6.595917          21    1.312605  22.147747   \n",
      "785871    hs or less   6.595917          35    1.312605  21.449707   \n",
      "\n",
      "            ind_wage     hpi  female  any_kids  black  hispanic  \n",
      "0       58771.657159  124.54       0         0      0         0  \n",
      "1       48179.067424  124.54       1         0      0         0  \n",
      "2       16368.472721  124.54       0         1      1         0  \n",
      "3       16368.472721  124.54       1         1      1         0  \n",
      "4       37101.999239  124.54       0         0      1         0  \n",
      "...              ...     ...     ...       ...    ...       ...  \n",
      "785867  37101.999239  116.19       1         1      0         0  \n",
      "785868  70798.119241  116.19       0         1      0         0  \n",
      "785869  16368.472721  116.19       1         0      0         0  \n",
      "785870  22112.724019  116.19       1         1      0         0  \n",
      "785871  40153.583596  116.19       1         0      0         0  \n",
      "\n",
      "[785872 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "hwd = pd.read_csv('hw_data.csv')\n",
    "print(hwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is focused on predicting wages. \n",
    "\n",
    "- **[1 point]** Create dummy variables for the multiclass variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hwd['hs_or_less'] = (hwd.degree == 'hs or less') * 1\n",
    "#hwd['some_college'] = (hwd.degree == 'some college') * 1\n",
    "#hwd['ba_or_more'] = (hwd.degree == 'ba or more') * 1\n",
    "#del hwd['degree']\n",
    "#hwd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I mistakenly used three different lines of code to produce dummy variables for the multiclass variable (degree). This was redundant as I could have used just two dummy variables ( 1 being otherwise for ba or more ). I also could have combined it into a shorter line of code using pd.get_dummies to accomplish this instead of creating dummies in the format that I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwd = hwd.join(pd.get_dummies(hwd.degree, drop_first = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key variables which are not used in analysis.\n",
    "We also no longer need the `degree` variable.\n",
    "\n",
    "* **[2 points]** In whatever combination of your choosing, **append** to the index or **drop** these four variables\n",
    "* **[1 point]** Print the bottom 3 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hwd['county']\n",
    "del hwd['occ2010']\n",
    "del hwd['ind1990']\n",
    "# deleted degree variable in previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>degree</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>hs or less</th>\n",
       "      <th>some college</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>785869</th>\n",
       "      <td>137743203</td>\n",
       "      <td>7.600902</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>0</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>11.182924</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785870</th>\n",
       "      <td>137753101</td>\n",
       "      <td>9.104980</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>21</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>22.147747</td>\n",
       "      <td>22112.724019</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785871</th>\n",
       "      <td>137756301</td>\n",
       "      <td>10.239960</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>35</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>21.449707</td>\n",
       "      <td>40153.583596</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  l_incwage  full_time        degree  l_density  experience  \\\n",
       "785869  137743203   7.600902          0    hs or less   6.595917           0   \n",
       "785870  137753101   9.104980          1  some college   6.595917          21   \n",
       "785871  137756301  10.239960          1    hs or less   6.595917          35   \n",
       "\n",
       "        puma_urate     occ_ba      ind_wage     hpi  female  any_kids  black  \\\n",
       "785869    1.312605  11.182924  16368.472721  116.19       1         0      0   \n",
       "785870    1.312605  22.147747  22112.724019  116.19       1         1      0   \n",
       "785871    1.312605  21.449707  40153.583596  116.19       1         0      0   \n",
       "\n",
       "        hispanic  hs or less  some college  \n",
       "785869         0           1             0  \n",
       "785870         0           0             1  \n",
       "785871         0           1             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwd.iloc[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## [5 Points] Summary Statistics \n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "**[1 point]** `.describe()` the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>hs or less</th>\n",
       "      <th>some college</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.858720e+05</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "      <td>785872.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.539035e+07</td>\n",
       "      <td>10.444697</td>\n",
       "      <td>0.810848</td>\n",
       "      <td>7.901147</td>\n",
       "      <td>21.497431</td>\n",
       "      <td>4.565357</td>\n",
       "      <td>37.408588</td>\n",
       "      <td>44485.232634</td>\n",
       "      <td>162.831250</td>\n",
       "      <td>0.485747</td>\n",
       "      <td>0.255293</td>\n",
       "      <td>0.105872</td>\n",
       "      <td>0.063462</td>\n",
       "      <td>0.293596</td>\n",
       "      <td>0.305323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.131173e+07</td>\n",
       "      <td>1.121583</td>\n",
       "      <td>0.391629</td>\n",
       "      <td>1.176867</td>\n",
       "      <td>13.217047</td>\n",
       "      <td>1.908056</td>\n",
       "      <td>30.099020</td>\n",
       "      <td>20384.552774</td>\n",
       "      <td>37.044526</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>0.436026</td>\n",
       "      <td>0.307674</td>\n",
       "      <td>0.243792</td>\n",
       "      <td>0.455409</td>\n",
       "      <td>0.460544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.010000e+02</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.284965</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.202896</td>\n",
       "      <td>1.256932</td>\n",
       "      <td>6229.223623</td>\n",
       "      <td>83.230000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.608078e+07</td>\n",
       "      <td>9.903488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.095976</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.280667</td>\n",
       "      <td>8.586593</td>\n",
       "      <td>28900.704927</td>\n",
       "      <td>133.180000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.522900e+07</td>\n",
       "      <td>10.596635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.972156</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.207127</td>\n",
       "      <td>25.116683</td>\n",
       "      <td>39496.468552</td>\n",
       "      <td>161.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.023246e+08</td>\n",
       "      <td>11.156251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.599400</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>5.479418</td>\n",
       "      <td>62.820715</td>\n",
       "      <td>58771.657159</td>\n",
       "      <td>185.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.377683e+08</td>\n",
       "      <td>13.478638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.510160</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>16.784090</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>112754.704290</td>\n",
       "      <td>287.230000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      l_incwage      full_time      l_density  \\\n",
       "count  7.858720e+05  785872.000000  785872.000000  785872.000000   \n",
       "mean   6.539035e+07      10.444697       0.810848       7.901147   \n",
       "std    4.131173e+07       1.121583       0.391629       1.176867   \n",
       "min    6.010000e+02       1.386294       0.000000       4.284965   \n",
       "25%    2.608078e+07       9.903488       1.000000       7.095976   \n",
       "50%    6.522900e+07      10.596635       1.000000       7.972156   \n",
       "75%    1.023246e+08      11.156251       1.000000       8.599400   \n",
       "max    1.377683e+08      13.478638       1.000000      11.510160   \n",
       "\n",
       "          experience     puma_urate         occ_ba       ind_wage  \\\n",
       "count  785872.000000  785872.000000  785872.000000  785872.000000   \n",
       "mean       21.497431       4.565357      37.408588   44485.232634   \n",
       "std        13.217047       1.908056      30.099020   20384.552774   \n",
       "min        -5.000000       0.202896       1.256932    6229.223623   \n",
       "25%        10.000000       3.280667       8.586593   28900.704927   \n",
       "50%        22.000000       4.207127      25.116683   39496.468552   \n",
       "75%        33.000000       5.479418      62.820715   58771.657159   \n",
       "max        58.000000      16.784090     100.000000  112754.704290   \n",
       "\n",
       "                 hpi         female       any_kids          black  \\\n",
       "count  785872.000000  785872.000000  785872.000000  785872.000000   \n",
       "mean      162.831250       0.485747       0.255293       0.105872   \n",
       "std        37.044526       0.499797       0.436026       0.307674   \n",
       "min        83.230000       0.000000       0.000000       0.000000   \n",
       "25%       133.180000       0.000000       0.000000       0.000000   \n",
       "50%       161.420000       0.000000       0.000000       0.000000   \n",
       "75%       185.250000       1.000000       1.000000       0.000000   \n",
       "max       287.230000       1.000000       1.000000       1.000000   \n",
       "\n",
       "            hispanic     hs or less   some college  \n",
       "count  785872.000000  785872.000000  785872.000000  \n",
       "mean        0.063462       0.293596       0.305323  \n",
       "std         0.243792       0.455409       0.460544  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000  \n",
       "75%         0.000000       1.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point]** Do any of the non-dummy variables have a mean *drastically* different than their medians?\n",
    "\n",
    "- If yes, produce a histogram of these variables and consider a logarithm transformation. If it appears appropriate to transform them, then do so. If not, then do not transform them.\n",
    "- If no, then do not produce any figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the variable 'occ_ba' seems to have a mean drastically different than its median\n",
    "\n",
    "#hwd.hist(column = 'occ_ba')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The answer to this question was no and there was simply no code to be ran here. I wasn't sure what qualified as 'drastically' different, so I used my intuition and figured occ_ba's mean (37.4) and median (25.11) values seemed quite far from one another when comparing the difference between mean and median values of the other non-dummy variables. I was incorrect. Because of this, I assume a drastic difference would be much more obvious than I originally believed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data for occ_ba is skewed, so I will take log transformation\n",
    "hwd['occ_ba'] = np.log(hwd['occ_ba'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **[1 point]** Produce a correlation matrix of the data. *Hint:* `df.corr()`\n",
    "- **[3 points]** What are the top three most correlated features with the label `l_incwage`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>hs or less</th>\n",
       "      <th>some college</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001453</td>\n",
       "      <td>0.017375</td>\n",
       "      <td>-0.133726</td>\n",
       "      <td>-0.004861</td>\n",
       "      <td>-0.095330</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>-0.216648</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>-0.002957</td>\n",
       "      <td>0.025661</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.006910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l_incwage</th>\n",
       "      <td>-0.001453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.561254</td>\n",
       "      <td>0.012788</td>\n",
       "      <td>0.259179</td>\n",
       "      <td>-0.108746</td>\n",
       "      <td>0.407380</td>\n",
       "      <td>0.424103</td>\n",
       "      <td>0.039968</td>\n",
       "      <td>-0.163449</td>\n",
       "      <td>0.120149</td>\n",
       "      <td>-0.077464</td>\n",
       "      <td>-0.060839</td>\n",
       "      <td>-0.283170</td>\n",
       "      <td>-0.103054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_time</th>\n",
       "      <td>0.017375</td>\n",
       "      <td>0.561254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.163330</td>\n",
       "      <td>-0.023964</td>\n",
       "      <td>0.162408</td>\n",
       "      <td>0.259565</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>-0.155095</td>\n",
       "      <td>0.059965</td>\n",
       "      <td>-0.007099</td>\n",
       "      <td>-0.008735</td>\n",
       "      <td>-0.085360</td>\n",
       "      <td>-0.062057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l_density</th>\n",
       "      <td>-0.133726</td>\n",
       "      <td>0.012788</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.073789</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.028919</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.528182</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>-0.032168</td>\n",
       "      <td>0.101015</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>-0.013554</td>\n",
       "      <td>-0.059257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <td>-0.004861</td>\n",
       "      <td>0.259179</td>\n",
       "      <td>0.163330</td>\n",
       "      <td>-0.073789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>-0.032896</td>\n",
       "      <td>0.077511</td>\n",
       "      <td>-0.031183</td>\n",
       "      <td>-0.011489</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>-0.021352</td>\n",
       "      <td>0.112883</td>\n",
       "      <td>0.037727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puma_urate</th>\n",
       "      <td>-0.095330</td>\n",
       "      <td>-0.108746</td>\n",
       "      <td>-0.023964</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.134519</td>\n",
       "      <td>-0.090986</td>\n",
       "      <td>-0.047324</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>-0.007941</td>\n",
       "      <td>0.178590</td>\n",
       "      <td>0.033540</td>\n",
       "      <td>0.105813</td>\n",
       "      <td>0.044917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occ_ba</th>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.407380</td>\n",
       "      <td>0.162408</td>\n",
       "      <td>0.028919</td>\n",
       "      <td>-0.032896</td>\n",
       "      <td>-0.134519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381134</td>\n",
       "      <td>0.039025</td>\n",
       "      <td>0.147109</td>\n",
       "      <td>0.031382</td>\n",
       "      <td>-0.081489</td>\n",
       "      <td>-0.088456</td>\n",
       "      <td>-0.499002</td>\n",
       "      <td>-0.135577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ind_wage</th>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.424103</td>\n",
       "      <td>0.259565</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.077511</td>\n",
       "      <td>-0.090986</td>\n",
       "      <td>0.381134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017792</td>\n",
       "      <td>-0.067388</td>\n",
       "      <td>0.045097</td>\n",
       "      <td>-0.045925</td>\n",
       "      <td>-0.052363</td>\n",
       "      <td>-0.254128</td>\n",
       "      <td>-0.051227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpi</th>\n",
       "      <td>-0.216648</td>\n",
       "      <td>0.039968</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.528182</td>\n",
       "      <td>-0.031183</td>\n",
       "      <td>-0.047324</td>\n",
       "      <td>0.039025</td>\n",
       "      <td>0.017792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.004341</td>\n",
       "      <td>-0.023895</td>\n",
       "      <td>0.101590</td>\n",
       "      <td>-0.014359</td>\n",
       "      <td>-0.038892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>0.000445</td>\n",
       "      <td>-0.163449</td>\n",
       "      <td>-0.155095</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>-0.011489</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.147109</td>\n",
       "      <td>-0.067388</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011891</td>\n",
       "      <td>0.051469</td>\n",
       "      <td>-0.004424</td>\n",
       "      <td>-0.076299</td>\n",
       "      <td>0.029855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any_kids</th>\n",
       "      <td>-0.002957</td>\n",
       "      <td>0.120149</td>\n",
       "      <td>0.059965</td>\n",
       "      <td>-0.032168</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>-0.007941</td>\n",
       "      <td>0.031382</td>\n",
       "      <td>0.045097</td>\n",
       "      <td>-0.004341</td>\n",
       "      <td>-0.011891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018192</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>-0.001170</td>\n",
       "      <td>-0.033549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>0.025661</td>\n",
       "      <td>-0.077464</td>\n",
       "      <td>-0.007099</td>\n",
       "      <td>0.101015</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.178590</td>\n",
       "      <td>-0.081489</td>\n",
       "      <td>-0.045925</td>\n",
       "      <td>-0.023895</td>\n",
       "      <td>0.051469</td>\n",
       "      <td>-0.018192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053203</td>\n",
       "      <td>0.033948</td>\n",
       "      <td>0.055673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanic</th>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.060839</td>\n",
       "      <td>-0.008735</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>-0.021352</td>\n",
       "      <td>0.033540</td>\n",
       "      <td>-0.088456</td>\n",
       "      <td>-0.052363</td>\n",
       "      <td>0.101590</td>\n",
       "      <td>-0.004424</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>-0.053203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079133</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs or less</th>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.283170</td>\n",
       "      <td>-0.085360</td>\n",
       "      <td>-0.013554</td>\n",
       "      <td>0.112883</td>\n",
       "      <td>0.105813</td>\n",
       "      <td>-0.499002</td>\n",
       "      <td>-0.254128</td>\n",
       "      <td>-0.014359</td>\n",
       "      <td>-0.076299</td>\n",
       "      <td>-0.001170</td>\n",
       "      <td>0.033948</td>\n",
       "      <td>0.079133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.427403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some college</th>\n",
       "      <td>-0.006910</td>\n",
       "      <td>-0.103054</td>\n",
       "      <td>-0.062057</td>\n",
       "      <td>-0.059257</td>\n",
       "      <td>0.037727</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>-0.135577</td>\n",
       "      <td>-0.051227</td>\n",
       "      <td>-0.038892</td>\n",
       "      <td>0.029855</td>\n",
       "      <td>-0.033549</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>-0.427403</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  l_incwage  full_time  l_density  experience  \\\n",
       "id            1.000000  -0.001453   0.017375  -0.133726   -0.004861   \n",
       "l_incwage    -0.001453   1.000000   0.561254   0.012788    0.259179   \n",
       "full_time     0.017375   0.561254   1.000000   0.003347    0.163330   \n",
       "l_density    -0.133726   0.012788   0.003347   1.000000   -0.073789   \n",
       "experience   -0.004861   0.259179   0.163330  -0.073789    1.000000   \n",
       "puma_urate   -0.095330  -0.108746  -0.023964   0.066038    0.012819   \n",
       "occ_ba        0.006343   0.407380   0.162408   0.028919   -0.032896   \n",
       "ind_wage      0.016733   0.424103   0.259565   0.011621    0.077511   \n",
       "hpi          -0.216648   0.039968   0.005690   0.528182   -0.031183   \n",
       "female        0.000445  -0.163449  -0.155095   0.003356   -0.011489   \n",
       "any_kids     -0.002957   0.120149   0.059965  -0.032168    0.049157   \n",
       "black         0.025661  -0.077464  -0.007099   0.101015    0.004730   \n",
       "hispanic     -0.032901  -0.060839  -0.008735   0.145714   -0.021352   \n",
       "hs or less   -0.002212  -0.283170  -0.085360  -0.013554    0.112883   \n",
       "some college -0.006910  -0.103054  -0.062057  -0.059257    0.037727   \n",
       "\n",
       "              puma_urate    occ_ba  ind_wage       hpi    female  any_kids  \\\n",
       "id             -0.095330  0.006343  0.016733 -0.216648  0.000445 -0.002957   \n",
       "l_incwage      -0.108746  0.407380  0.424103  0.039968 -0.163449  0.120149   \n",
       "full_time      -0.023964  0.162408  0.259565  0.005690 -0.155095  0.059965   \n",
       "l_density       0.066038  0.028919  0.011621  0.528182  0.003356 -0.032168   \n",
       "experience      0.012819 -0.032896  0.077511 -0.031183 -0.011489  0.049157   \n",
       "puma_urate      1.000000 -0.134519 -0.090986 -0.047324  0.015600 -0.007941   \n",
       "occ_ba         -0.134519  1.000000  0.381134  0.039025  0.147109  0.031382   \n",
       "ind_wage       -0.090986  0.381134  1.000000  0.017792 -0.067388  0.045097   \n",
       "hpi            -0.047324  0.039025  0.017792  1.000000 -0.006029 -0.004341   \n",
       "female          0.015600  0.147109 -0.067388 -0.006029  1.000000 -0.011891   \n",
       "any_kids       -0.007941  0.031382  0.045097 -0.004341 -0.011891  1.000000   \n",
       "black           0.178590 -0.081489 -0.045925 -0.023895  0.051469 -0.018192   \n",
       "hispanic        0.033540 -0.088456 -0.052363  0.101590 -0.004424  0.018073   \n",
       "hs or less      0.105813 -0.499002 -0.254128 -0.014359 -0.076299 -0.001170   \n",
       "some college    0.044917 -0.135577 -0.051227 -0.038892  0.029855 -0.033549   \n",
       "\n",
       "                 black  hispanic  hs or less  some college  \n",
       "id            0.025661 -0.032901   -0.002212     -0.006910  \n",
       "l_incwage    -0.077464 -0.060839   -0.283170     -0.103054  \n",
       "full_time    -0.007099 -0.008735   -0.085360     -0.062057  \n",
       "l_density     0.101015  0.145714   -0.013554     -0.059257  \n",
       "experience    0.004730 -0.021352    0.112883      0.037727  \n",
       "puma_urate    0.178590  0.033540    0.105813      0.044917  \n",
       "occ_ba       -0.081489 -0.088456   -0.499002     -0.135577  \n",
       "ind_wage     -0.045925 -0.052363   -0.254128     -0.051227  \n",
       "hpi          -0.023895  0.101590   -0.014359     -0.038892  \n",
       "female        0.051469 -0.004424   -0.076299      0.029855  \n",
       "any_kids     -0.018192  0.018073   -0.001170     -0.033549  \n",
       "black         1.000000 -0.053203    0.033948      0.055673  \n",
       "hispanic     -0.053203  1.000000    0.079133      0.001945  \n",
       "hs or less    0.033948  0.079133    1.000000     -0.427403  \n",
       "some college  0.055673  0.001945   -0.427403      1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = hwd.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 3 most correlated features with the label 'l_incwage' are:\n",
    "# 1. full_time (0.561254)\n",
    "# 2. ind_wage (0.424103)\n",
    "# 3. occ_ba (0.407380)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## [10 Points] Figures\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "The Mincer equation suggests that wages are a function of education and a quadratic of experience:\n",
    "$$\n",
    "ln(w_i) = ln(w_0) + Ed_i \\rho + Exp_i\\beta_1  + Exp_I^2\\beta_2 + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Do you find evidence of a quadratic relationship between log wages and experience?\n",
    "Produce a \n",
    "\n",
    "- **[2 points]** binned scatter plot of log wages versus experience\n",
    "- **[1 point]** with 50 bins\n",
    "- **[1 point]** with axis labels\n",
    "- **[1 point]** with a tite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Binned scatter plot\n",
    "n = 50\n",
    "bin_mean, bin_edge, bin_number = binned_statistic(hwd.experience, np.log(hwd.l_incwage), bins = n)\n",
    "\n",
    "x = np.average([bin_edge[:-1], bin_edge[1:]], axis = 0)\n",
    "\n",
    "plt.scatter(x, bin_mean, label = '%d bins' % n)\n",
    "plt.xlabel('experience')\n",
    "plt.ylabel('log wage')\n",
    "plt.title('log wage vs experience')\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "My answer for this one was correct. I only got points off because the grader could not see the bins argument within my function that created the scatter plot. I will ensure for future assignments that I export my Jupyter notebook as an html file and then convert it into a pdf file. This way the grader will be able to see the entirety of my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 point]** Do you find evidence of a quadratic relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By the looks of the binned scatterplot, there is evidence of a quadratic relationship.\n",
    "# The data points dip up and back down, which slightly resembles a parabola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "# Application - Regression \n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "## [10 Points] Data Prep\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "Append to the data set\n",
    "\n",
    "- **[3 points]** a quadratic, cubic, and quartic transformation of experience\n",
    "- **[1 point]** an interaction between female and any children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>degree</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>hs or less</th>\n",
       "      <th>some college</th>\n",
       "      <th>experience_quad</th>\n",
       "      <th>experience_cubic</th>\n",
       "      <th>experience_quartic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>9.862666</td>\n",
       "      <td>0</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.695985</td>\n",
       "      <td>6</td>\n",
       "      <td>5.917822</td>\n",
       "      <td>2.150202</td>\n",
       "      <td>58771.657159</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>216</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>9.857444</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.695985</td>\n",
       "      <td>5</td>\n",
       "      <td>5.917822</td>\n",
       "      <td>2.266533</td>\n",
       "      <td>48179.067424</td>\n",
       "      <td>124.54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>125</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>801</td>\n",
       "      <td>10.203592</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.758590</td>\n",
       "      <td>7</td>\n",
       "      <td>5.454464</td>\n",
       "      <td>1.380550</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>343</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>802</td>\n",
       "      <td>9.798127</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>7.758590</td>\n",
       "      <td>7</td>\n",
       "      <td>5.454464</td>\n",
       "      <td>1.945288</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>124.54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>343</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2301</td>\n",
       "      <td>11.289782</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>5.786897</td>\n",
       "      <td>18</td>\n",
       "      <td>3.103390</td>\n",
       "      <td>1.967492</td>\n",
       "      <td>37101.999239</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>5832</td>\n",
       "      <td>104976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785867</th>\n",
       "      <td>137743201</td>\n",
       "      <td>8.517193</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>34</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>3.168046</td>\n",
       "      <td>37101.999239</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1156</td>\n",
       "      <td>39304</td>\n",
       "      <td>1336336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785868</th>\n",
       "      <td>137743202</td>\n",
       "      <td>11.156251</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>30</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>2.774552</td>\n",
       "      <td>70798.119241</td>\n",
       "      <td>116.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>27000</td>\n",
       "      <td>810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785869</th>\n",
       "      <td>137743203</td>\n",
       "      <td>7.600902</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>0</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>2.414388</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785870</th>\n",
       "      <td>137753101</td>\n",
       "      <td>9.104980</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>21</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>3.097736</td>\n",
       "      <td>22112.724019</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>194481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785871</th>\n",
       "      <td>137756301</td>\n",
       "      <td>10.239960</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>35</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>3.065711</td>\n",
       "      <td>40153.583596</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1225</td>\n",
       "      <td>42875</td>\n",
       "      <td>1500625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785872 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  l_incwage  full_time        degree  l_density  experience  \\\n",
       "0             601   9.862666          0  some college   7.695985           6   \n",
       "1             602   9.857444          1  some college   7.695985           5   \n",
       "2             801  10.203592          1  some college   7.758590           7   \n",
       "3             802   9.798127          0    hs or less   7.758590           7   \n",
       "4            2301  11.289782          1    hs or less   5.786897          18   \n",
       "...           ...        ...        ...           ...        ...         ...   \n",
       "785867  137743201   8.517193          0    hs or less   6.595917          34   \n",
       "785868  137743202  11.156251          1    hs or less   6.595917          30   \n",
       "785869  137743203   7.600902          0    hs or less   6.595917           0   \n",
       "785870  137753101   9.104980          1  some college   6.595917          21   \n",
       "785871  137756301  10.239960          1    hs or less   6.595917          35   \n",
       "\n",
       "        puma_urate    occ_ba      ind_wage     hpi  female  any_kids  black  \\\n",
       "0         5.917822  2.150202  58771.657159  124.54       0         0      0   \n",
       "1         5.917822  2.266533  48179.067424  124.54       1         0      0   \n",
       "2         5.454464  1.380550  16368.472721  124.54       0         1      1   \n",
       "3         5.454464  1.945288  16368.472721  124.54       1         1      1   \n",
       "4         3.103390  1.967492  37101.999239  124.54       0         0      1   \n",
       "...            ...       ...           ...     ...     ...       ...    ...   \n",
       "785867    1.312605  3.168046  37101.999239  116.19       1         1      0   \n",
       "785868    1.312605  2.774552  70798.119241  116.19       0         1      0   \n",
       "785869    1.312605  2.414388  16368.472721  116.19       1         0      0   \n",
       "785870    1.312605  3.097736  22112.724019  116.19       1         1      0   \n",
       "785871    1.312605  3.065711  40153.583596  116.19       1         0      0   \n",
       "\n",
       "        hispanic  hs or less  some college  experience_quad  experience_cubic  \\\n",
       "0              0           0             1               36               216   \n",
       "1              0           0             1               25               125   \n",
       "2              0           0             1               49               343   \n",
       "3              0           1             0               49               343   \n",
       "4              0           1             0              324              5832   \n",
       "...          ...         ...           ...              ...               ...   \n",
       "785867         0           1             0             1156             39304   \n",
       "785868         0           1             0              900             27000   \n",
       "785869         0           1             0                0                 0   \n",
       "785870         0           0             1              441              9261   \n",
       "785871         0           1             0             1225             42875   \n",
       "\n",
       "        experience_quartic  \n",
       "0                     1296  \n",
       "1                      625  \n",
       "2                     2401  \n",
       "3                     2401  \n",
       "4                   104976  \n",
       "...                    ...  \n",
       "785867             1336336  \n",
       "785868              810000  \n",
       "785869                   0  \n",
       "785870              194481  \n",
       "785871             1500625  \n",
       "\n",
       "[785872 rows x 19 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quadratic\n",
    "\n",
    "hwd['experience_quad'] = (hwd['experience'])**2\n",
    "\n",
    "# Cubic\n",
    "\n",
    "hwd['experience_cubic'] = (hwd['experience'])**3\n",
    "\n",
    "# Quartic\n",
    "\n",
    "hwd['experience_quartic'] = (hwd['experience'])**4\n",
    "\n",
    "# Check to see it worked\n",
    "hwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>degree</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>hs or less</th>\n",
       "      <th>some college</th>\n",
       "      <th>experience_quad</th>\n",
       "      <th>experience_cubic</th>\n",
       "      <th>experience_quartic</th>\n",
       "      <th>female:any_kids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>9.862666</td>\n",
       "      <td>0</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.695985</td>\n",
       "      <td>6</td>\n",
       "      <td>5.917822</td>\n",
       "      <td>2.150202</td>\n",
       "      <td>58771.657159</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>216</td>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>9.857444</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.695985</td>\n",
       "      <td>5</td>\n",
       "      <td>5.917822</td>\n",
       "      <td>2.266533</td>\n",
       "      <td>48179.067424</td>\n",
       "      <td>124.54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>125</td>\n",
       "      <td>625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>801</td>\n",
       "      <td>10.203592</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.758590</td>\n",
       "      <td>7</td>\n",
       "      <td>5.454464</td>\n",
       "      <td>1.380550</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>343</td>\n",
       "      <td>2401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>802</td>\n",
       "      <td>9.798127</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>7.758590</td>\n",
       "      <td>7</td>\n",
       "      <td>5.454464</td>\n",
       "      <td>1.945288</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>124.54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>343</td>\n",
       "      <td>2401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2301</td>\n",
       "      <td>11.289782</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>5.786897</td>\n",
       "      <td>18</td>\n",
       "      <td>3.103390</td>\n",
       "      <td>1.967492</td>\n",
       "      <td>37101.999239</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>5832</td>\n",
       "      <td>104976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785867</th>\n",
       "      <td>137743201</td>\n",
       "      <td>8.517193</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>34</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>3.168046</td>\n",
       "      <td>37101.999239</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1156</td>\n",
       "      <td>39304</td>\n",
       "      <td>1336336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785868</th>\n",
       "      <td>137743202</td>\n",
       "      <td>11.156251</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>30</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>2.774552</td>\n",
       "      <td>70798.119241</td>\n",
       "      <td>116.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>27000</td>\n",
       "      <td>810000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785869</th>\n",
       "      <td>137743203</td>\n",
       "      <td>7.600902</td>\n",
       "      <td>0</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>0</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>2.414388</td>\n",
       "      <td>16368.472721</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785870</th>\n",
       "      <td>137753101</td>\n",
       "      <td>9.104980</td>\n",
       "      <td>1</td>\n",
       "      <td>some college</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>21</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>3.097736</td>\n",
       "      <td>22112.724019</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>441</td>\n",
       "      <td>9261</td>\n",
       "      <td>194481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785871</th>\n",
       "      <td>137756301</td>\n",
       "      <td>10.239960</td>\n",
       "      <td>1</td>\n",
       "      <td>hs or less</td>\n",
       "      <td>6.595917</td>\n",
       "      <td>35</td>\n",
       "      <td>1.312605</td>\n",
       "      <td>3.065711</td>\n",
       "      <td>40153.583596</td>\n",
       "      <td>116.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1225</td>\n",
       "      <td>42875</td>\n",
       "      <td>1500625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785872 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  l_incwage  full_time        degree  l_density  experience  \\\n",
       "0             601   9.862666          0  some college   7.695985           6   \n",
       "1             602   9.857444          1  some college   7.695985           5   \n",
       "2             801  10.203592          1  some college   7.758590           7   \n",
       "3             802   9.798127          0    hs or less   7.758590           7   \n",
       "4            2301  11.289782          1    hs or less   5.786897          18   \n",
       "...           ...        ...        ...           ...        ...         ...   \n",
       "785867  137743201   8.517193          0    hs or less   6.595917          34   \n",
       "785868  137743202  11.156251          1    hs or less   6.595917          30   \n",
       "785869  137743203   7.600902          0    hs or less   6.595917           0   \n",
       "785870  137753101   9.104980          1  some college   6.595917          21   \n",
       "785871  137756301  10.239960          1    hs or less   6.595917          35   \n",
       "\n",
       "        puma_urate    occ_ba      ind_wage     hpi  female  any_kids  black  \\\n",
       "0         5.917822  2.150202  58771.657159  124.54       0         0      0   \n",
       "1         5.917822  2.266533  48179.067424  124.54       1         0      0   \n",
       "2         5.454464  1.380550  16368.472721  124.54       0         1      1   \n",
       "3         5.454464  1.945288  16368.472721  124.54       1         1      1   \n",
       "4         3.103390  1.967492  37101.999239  124.54       0         0      1   \n",
       "...            ...       ...           ...     ...     ...       ...    ...   \n",
       "785867    1.312605  3.168046  37101.999239  116.19       1         1      0   \n",
       "785868    1.312605  2.774552  70798.119241  116.19       0         1      0   \n",
       "785869    1.312605  2.414388  16368.472721  116.19       1         0      0   \n",
       "785870    1.312605  3.097736  22112.724019  116.19       1         1      0   \n",
       "785871    1.312605  3.065711  40153.583596  116.19       1         0      0   \n",
       "\n",
       "        hispanic  hs or less  some college  experience_quad  experience_cubic  \\\n",
       "0              0           0             1               36               216   \n",
       "1              0           0             1               25               125   \n",
       "2              0           0             1               49               343   \n",
       "3              0           1             0               49               343   \n",
       "4              0           1             0              324              5832   \n",
       "...          ...         ...           ...              ...               ...   \n",
       "785867         0           1             0             1156             39304   \n",
       "785868         0           1             0              900             27000   \n",
       "785869         0           1             0                0                 0   \n",
       "785870         0           0             1              441              9261   \n",
       "785871         0           1             0             1225             42875   \n",
       "\n",
       "        experience_quartic  female:any_kids  \n",
       "0                     1296                0  \n",
       "1                      625                0  \n",
       "2                     2401                0  \n",
       "3                     2401                1  \n",
       "4                   104976                0  \n",
       "...                    ...              ...  \n",
       "785867             1336336                1  \n",
       "785868              810000                0  \n",
       "785869                   0                0  \n",
       "785870              194481                1  \n",
       "785871             1500625                0  \n",
       "\n",
       "[785872 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwd['female:any_kids'] = hwd.female * hwd.any_kids\n",
    "hwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point]** Print the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(hwd.count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For some reason, I thought that summing up the result of hwd.count() would return the number of observations in the data frame. The count function counts all of the non NA/ null observations of the data frame and the sum function adds up all of the values itself. This obviously returned a number much bigger than what it should have been. I could have used any of the three functions shown below to count the number of observations. The .shape() function returns a tuple of the shape of the data frame and the len function returns the number of items in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785872, 20)\n",
      "785872\n",
      "785872\n"
     ]
    }
   ],
   "source": [
    "print(hwd.shape)  #or\n",
    "print(hwd.shape[0])  #or\n",
    "print(len(hwd.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the non-standardized label.\n",
    "\n",
    "- **[1 point]** Create `x` and `y`.\n",
    "\n",
    "Create `x_train`, `x_test`, `y_train`, `y_test` with a sample split using\n",
    "\n",
    "- **[1 point]** a train set size of 5%\n",
    "- **[1 point]** a random seed of 490\n",
    "\n",
    "Then \n",
    "\n",
    "- **[1 point]** standardize `x_train` and `x_test`\n",
    "- **[1 point]** add an intercept to all feature objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-standardized label\n",
    "# y = hwd['l_incwage']\n",
    "# x = hwd.drop(columns = 'l_incwage')\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.05, random_state = 490)\n",
    "\n",
    "# # Standardize x_train and x_test\n",
    "# x_train_std = x_train.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "# x_test_std  = x_test.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "\n",
    "# x_train_std = sm.add_constant(x_train_std)\n",
    "# x_test_std  = sm.add_constant(x_test_std)\n",
    "# x_train     = sm.add_constant(x_train)\n",
    "# x_test      = sm.add_constant(x_test)\n",
    "\n",
    "x = hwd.drop(columns = 'l_incwage')\n",
    "y = hwd['l_incwage']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.05, random_state = 490)\n",
    "                                                    \n",
    "# Standardize x_train and x_test\n",
    "x_train_std = x_train.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "x_test_std = x_test.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "\n",
    "x_train_std = sm.add_constant(x_train_std)\n",
    "x_test_std = sm.add_constant(x_test_std)\n",
    "x_train = sm.add_constant(x_train)\n",
    "x_test = sm.add_constant(x_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Again, this code was correct but I received points off because the grader was not able to see the entirety of the functions/arguments. Making this same mistake again, I will now be completely sure that I follow the proper procedure of exporting the file as html before converting to pdf so the grader can see all of my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "## [2 Points] Null Model \n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "- **[1 point]** Obtain the test RMSE of the null model\n",
    "- **[1 point]** print the test RMSE of the null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_null = np.sqrt(np.mean((y_test - np.mean(y_train))**2))\n",
    "rmse_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********\n",
    "## [4 Points] Backward Selection\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "**[1 point]** Perform backward selection using the **non-standardized features** with a threshold of 0.05.\n",
    "\n",
    "- **[1 point]** produce a summary of the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backselect_model = sm.OLS(y_train, x_train).fit()\n",
    "#print(backselect_model.summary2())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For this question, I used the wrong function for producing the summary of the selected model. The summary2 function is used for logistic regression problems, which is not what we are doing here. We are working with the null model here, so simply using the regular .summary() function would be correct. I also failed to include the .drop(columns = [] argument before the .fit() function in my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backselect_model = sm.OLS(y_train, x_train.drop.fit())\n",
    "backselect_model.summary()                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[1 point]** obtain the test RMSE\n",
    "- **[1 point]** print the test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_backward = np.sqrt(np.mean((y_test - backselect_model.predict(x_test))**2))\n",
    "rmse_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "## [10 Points] Regularization\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "Identify the optimal $\\alpha$ and Lasso weight on the standardized features.\n",
    "- **[1 point]** Using 5-fold grid search cross-validation\n",
    "- **[1 point]** Add the `%%time` magic command in the first line of the CV code chunk\n",
    "- **[1 point]** use a grid over $\\alpha$ by powers of 10 no smaller than $10^{-10}$\n",
    "- **[1 point]** use a grid of the Lasso weight with a step size of 0.2\n",
    "- **[1 point]** use a random state of 490\n",
    "    - Ignore convergence warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# hwd_grid = {\n",
    "#     'alpha': 10.0**np.arange(-9, -5, 10),\n",
    "#     'l1_ratio':  np.arange(0, 1, step = 0.2)\n",
    "# }\n",
    "\n",
    "# lr_cv = lm.ElasticNet(fit_intercept = False, normalize = False, random_state = 490)\n",
    "                             \n",
    "# grid_search = GridSearchCV(lr_cv, hwd_grid, \n",
    "#                           cv = 5, \n",
    "#                           scoring = 'accuracy',\n",
    "#                           n_jobs = 10)\n",
    "# grid_search.fit(x_train_std, y_train)\n",
    "# best = grid_search.best_params_\n",
    "# best"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To no surprise, the entirety of my code was not displayed again, but this time I lost points where I set the scoring argument equal to 'accuracy'. The scoring argument would be set equal to 'accuracy' if we were performing logit regularization because we would be interested in the accuracy of the logit model prediction. In this case, we are performing regularization using standard OLS. With OLS, we would like scoring be set equal to neg_root_mean_squared_error because that is the value we will use to estimate the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hwd_grid = {'alpha': 10.**np.arange(-10, -1, 1),\n",
    "           'l1_ratio': np.arange(0, 1, 0.2)}\n",
    "\n",
    "lr_cv = lm.ElasticNet(fit_intercept = False, random_state = 490)\n",
    "\n",
    "grid_search = GridSearchCV(lr_cv, hwd_grid, \n",
    "                           cv = 5, \n",
    "                           scoring = 'neg_root_mean_squared_error',\n",
    "                           n_jobs = 10)\n",
    "grid_search.fit(x_train_std, y_train)\n",
    "best = grid_search.best_params_\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the identified optimal hyperparameters\n",
    "\n",
    "- **[1 point]** refit the model with `statsmodels` on the standardize features\n",
    "- **[1 point]** if your identified $\\alpha$ was $10^{-10}$, then set $\\alpha$ to zero\n",
    "- **[1 point]** print the parameters\n",
    "- **[1 point]** obtain the rmse\n",
    "- **[1 point]** print the rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso = sm.OLS(y_train, x_train_std).fit_regularized(alpha = best['alpha'], L1_wt = 0.2)\n",
    "#print(lasso.params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I received points off because the grader could not see, but I also got the problem incorrect because I used alpha and L1_wt. The alpha is 10^-10, so alpha should be set to zero because the lasso weight is irrelevant. Alpha in regularization combats overfitting by constraining the size of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = sm.OLS(y_train, x_train_std).fit()\n",
    "print(lasso.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lasso = np.sqrt(np.mean((y_test - lasso.predict(x_test_std))**2))\n",
    "rmse_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you should realize that neither backward selection nor regularization have reduced the number of features. \n",
    "I won't ask you to refit using \"selected\" variables to avoid the redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "## [4 Points] Comparison\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "**[2 points]** Obtain the percent decrease in RMSE for each model relative to the null model. **DO NOT PRINT THEM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Selection Model\n",
    "perc_dec_backward = (((rmse_backward - rmse_null)/ rmse_null) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Model\n",
    "perc_dec_lasso = (((rmse_lasso - rmse_null)/ rmse_null) *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one `print()` statement, print the\n",
    "\n",
    "- **[1 point]** percent improvement of each model round to two basis points\n",
    "- **[1 point]** a new line for each model's RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(round(perc_dec_backward, 2))\n",
    "#print(round(perc_dec_lasso, 2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I got this question wrong because I failed to notice that it asked for only one print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Backward Selection RMSE: %.2f\\nRegularization RMSE: %.2f' % (perc_dec_backward, perc_dec_lasso))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
