{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homework 4 - Rise of the Machines\n",
    "### Your Name: Alyssa Kern\n",
    "\n",
    "**COLLABORATED WITH:_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "**Submit a PDF export of your notebook (100% PENALTY IF NO PDF IS SUBMITTED)** \n",
    "\n",
    "* File > Export Notebook to HTML (you cannot upload .html to Compass), open .html in browser, print to PDF\n",
    "    - You need to use this option if you have math with `\\begin{align} ... \\end{align}`.\n",
    "    - Ensure your code is not cut off\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "\n",
    "***********************************************************************************************\n",
    "# Intuition\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "**Points are not awarded for correct answers**. \n",
    "Instead, points are awarded for explainations of why you are correct or why you are wrong.\n",
    "You must attempt to answer the question to receive any points.\n",
    "Points for intuition questions are pass/fail.\n",
    "\n",
    "Answers such as \"I am correct/wrong\" or \"because the textbook says so\" are invalid. \n",
    "Show us you know what you are talking about: explain.\n",
    "\n",
    "\n",
    "## [5 Points] HoML Ch5 Q1\n",
    "\n",
    "What is the fundamental idea behind Support Vector Machines (classification)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "* \"widest street\"\n",
    "* \"minimizing margin violation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are you correct or wrong?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SMV's essentially create the 'widest street' possible in order to seperate the data points between classes. This creates a clear seperation between classes which makes the SMV perform very well. This is because SMV's act to minimize margin violations between classes. Margin violation involves choosing a hyperplane that allows some data points to stay on the 'wrong' side of the hyperplane or allowing them to stay between the margin and the 'right' side of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 Points] HoML Ch5 Q3\n",
    "\n",
    "Why is it important to scale the inputs when using SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "* variables with larger values will dominate the decision threshold/street\n",
    "* not fit the best (widest) possible street between classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are you correct or wrong?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVM's optimize by minimizing the decision vector/margin violation. The optimal hyperplane we are trying to fit is significantly influenced by the scale of the input features. Because of this, it's important to scale the inputs when using SVM's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "# Theory\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "***********\n",
    "## [5 Point] Maximial Margin Classifier Part 1\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "What kind of kernel does a maximal margin classifier use?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since the maximal margin classifier is a linear function that represents a hyperplane to seperate data, we would use the sklearn.svm.LinearSVC/R kernel. This is because it is computationally optimized for linear kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "## [5 Point] Maximial Margin Classifier Part 2\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "When does a maximal margin classifier fail?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A maximal margin classifier fails when the data it is working with is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "## [10 Point] Multiple Classes\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "Using an SVM to predict an 8 class label...\n",
    "\n",
    "**[5 points]** If we are using one vs. one classification, how many models are required to make a prediction?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A formula for calculating the number of models required to make a prediction using one vs. one classification is:\n",
    "\n",
    "( # of classes ( # of classes - 1)) / 2\n",
    "\n",
    "So\n",
    "\n",
    "( 8 ( 8 - 1)) / 2 = 28\n",
    "\n",
    "So we need 28 models when using an SVM to predict an 8 class label using one vs. one classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5 points]** If we are using one vs. rest classification, how many models are required to make a prediction?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we are using an SVM to predict an 8 class label and are using one vs. rest classification, we would simply need as many models as there are classes. In this case, using an SVM to predict an 8 class label would require 8 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "# Application\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "You will get more independence in this section.\n",
    "We will be using SVMs for regression (*use the continuous label!*).\n",
    "Please reference the `sklearn` API reference to determine which functions we need to use: https://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "********\n",
    "## [9 Points] Preliminaries\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "Checklist:\n",
    "\n",
    "- load all necessary packages and functions\n",
    "- **[3 points]** deal with variables that should be in the index\n",
    "- **[2 points]** deal with categorical variables\n",
    "- **[2 points]** perform a train-test split with a training size of 1/100 and random state of 490\n",
    "- **[2 points]** standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# processing\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# alogirthms\n",
    "from sklearn.svm import SVR, LinearSVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>degree</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county</th>\n",
       "      <th>occ2010</th>\n",
       "      <th>ind1990</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <th>3600</th>\n",
       "      <th>831</th>\n",
       "      <td>601</td>\n",
       "      <td>9.862666</td>\n",
       "      <td>0</td>\n",
       "      <td>some college</td>\n",
       "      <td>7.695985</td>\n",
       "      <td>6</td>\n",
       "      <td>5.917822</td>\n",
       "      <td>8.586593</td>\n",
       "      <td>58771.657159</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  l_incwage  full_time        degree  l_density  \\\n",
       "county occ2010 ind1990                                                       \n",
       "1097   3600    831      601   9.862666          0  some college   7.695985   \n",
       "\n",
       "                        experience  puma_urate    occ_ba      ind_wage  \\\n",
       "county occ2010 ind1990                                                   \n",
       "1097   3600    831               6    5.917822  8.586593  58771.657159   \n",
       "\n",
       "                           hpi  female  any_kids  black  hispanic  \n",
       "county occ2010 ind1990                                             \n",
       "1097   3600    831      124.54       0         0      0         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('hw_data.csv', index_col = ['county', 'occ2010', 'ind1990'])\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>l_incwage</th>\n",
       "      <th>full_time</th>\n",
       "      <th>l_density</th>\n",
       "      <th>experience</th>\n",
       "      <th>puma_urate</th>\n",
       "      <th>occ_ba</th>\n",
       "      <th>ind_wage</th>\n",
       "      <th>hpi</th>\n",
       "      <th>female</th>\n",
       "      <th>any_kids</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>hs or less</th>\n",
       "      <th>some college</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county</th>\n",
       "      <th>occ2010</th>\n",
       "      <th>ind1990</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <th>3600</th>\n",
       "      <th>831</th>\n",
       "      <td>601</td>\n",
       "      <td>9.862666</td>\n",
       "      <td>0</td>\n",
       "      <td>7.695985</td>\n",
       "      <td>6</td>\n",
       "      <td>5.917822</td>\n",
       "      <td>8.586593</td>\n",
       "      <td>58771.657159</td>\n",
       "      <td>124.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  l_incwage  full_time  l_density  experience  \\\n",
       "county occ2010 ind1990                                                     \n",
       "1097   3600    831      601   9.862666          0   7.695985           6   \n",
       "\n",
       "                        puma_urate    occ_ba      ind_wage     hpi  female  \\\n",
       "county occ2010 ind1990                                                       \n",
       "1097   3600    831        5.917822  8.586593  58771.657159  124.54       0   \n",
       "\n",
       "                        any_kids  black  hispanic  hs or less  some college  \n",
       "county occ2010 ind1990                                                       \n",
       "1097   3600    831             0      0         0           0             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = df.drop(columns = 'degree')\n",
    "right = pd.get_dummies(df['degree'], drop_first = True)\n",
    "\n",
    "df_prepped = pd.concat([left, right], axis = 1)\n",
    "df_prepped.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped['l_incwage']\n",
    "x = df_prepped.drop(columns = 'l_incwage')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 1/100 , random_state = 490)\n",
    "\n",
    "x_train_std = pd.DataFrame(StandardScaler().fit(x_train).transform(x_train),\n",
    "                           columns = x_train.columns,\n",
    "                           index = x_train.index)\n",
    "\n",
    "x_test_std = pd.DataFrame(StandardScaler().fit(x_test).transform(x_test),\n",
    "                          columns = x_test.columns, \n",
    "                          index = x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "## [4 Points] Null Model\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "- **[2 points]** fit the null model\n",
    "- **[2 points]** save the rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0320379838923207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yhat_null = y_train.value_counts().index[0]\n",
    "#acc_null = np.mean(yhat_null == y_test)\n",
    "#acc_null\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I failed to include np.mean to use the average value of the y_train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_null = np.mean(y_train)\n",
    "rmse_null = np.sqrt(np.mean( (y_test - yhat)**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "## [16 Points] Polynomial Kernel\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "Using a polynomial kernel, use 5-fold cross-validation grid search to identify optimal values of\n",
    "\n",
    "- **[4 points]** `degree` - up to a third order\n",
    "- **[4 points]** `C` - to the closest magnitude ($10^n$)\n",
    "- **[4 points]** `epsilon` - to the closest magnitude ($10^n$)\n",
    "- `coef0` - IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 3.65 s, total: 1min 30s\n",
      "Wall time: 1min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10.0, 'degree': 3, 'epsilon': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'C': 10.**np.arange(-1, 2, step = 1),\n",
    "    'degree': [1, 2, 3],\n",
    "    'epsilon': 10.**np.arange(-1, 1, step = 1),\n",
    "\n",
    "}\n",
    "\n",
    "svmr_cv = SVR(kernel = 'poly')\n",
    "\n",
    "grid_search = GridSearchCV(svmr_cv, param_grid,\n",
    "                          cv = 5, \n",
    "                          scoring = 'neg_root_mean_squared_error').fit(x_train, y_train)\n",
    "\n",
    "best = grid_search.best_params_\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** Refit on the whole training data using the cross-validated hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 15\n",
    "#svmr_poly = SVR(\n",
    "                     #C = best['C'],\n",
    "                     #epsilon = best['epsilon'],\n",
    "                     #degree = best['degree'])\n",
    "#svmr_poly = svmr_poly.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I forgot to add the argument 'kernel = 'poly''. This was necessary because we are working with a polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmr_poly = SVR(kernel = 'poly',\n",
    "               degree = best['degree'],\n",
    "               C = best['C'],\n",
    "               epsilon = best['epsilon'],)\n",
    "\n",
    "svmr_poly.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** Obtain the RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016615228635684653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lecture 15\n",
    "#acc_poly = svmr_poly.score(x_test, y_test)\n",
    "#acc_poly\n",
    "# try this - if it doesnt work, try something else (i dont remember what TA said)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used .score instead of .predict. We need to use .predict because we want to predict the labels of the data values on the basis of the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yhat_poly = svmr_poly.predict(x_test)\n",
    "rmse_poly = np.sqrt(np.mean( (y_test - yhat_poly)**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "## [12 Points] RBF\n",
    "[TOP](#Homework-4---Rise-of-the-Machines)\n",
    "\n",
    "Using a radial basis function kernel, use 5-fold cross-validation grid search to identify optimal values of\n",
    "\n",
    "- **[4 points]** `C` - to the closest magnitude ($10^n$)\n",
    "- **[4 points]** `gamma` - to the closest magnitude ($10^n$)\n",
    "- `epsilon` - IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 6.35 s, total: 3min 4s\n",
      "Wall time: 3min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10.0, 'gamma': 1e-06}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "param_grid_2 = {\n",
    "    'C': 10.**np.arange(-1, 4, step = 1),\n",
    "    'gamma': 10.**np.arange(-7, -3, step = 1)\n",
    "}\n",
    "\n",
    "svmr_cv_2 = SVR(kernel = 'rbf')\n",
    "\n",
    "grid_search_2 = GridSearchCV(svmr_cv_2, param_grid_2,\n",
    "                          cv = 5, \n",
    "                          scoring = 'neg_root_mean_squared_error').fit(x_train, y_train)\n",
    "\n",
    "best_2 = grid_search_2.best_params_\n",
    "best_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** Refit on the whole training data using the cross-validated hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 15\n",
    "cvr_rbf = SVR( kernel = 'rbf',\n",
    "             C = best_2['C'],\n",
    "             gamma = best_2['gamma'])\n",
    "cvr_rbf = cvr_rbf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** Obtain the RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0012407504525924207"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#acc_rbf = cvr_rbf.score(x_test, y_test)\n",
    "#acc_rbf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similar with the last problem I got wrong, I used .score instead of .predict. This is the same reasoning as my last answer that was incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yhat_poly = svmr_poly.predict(x_test)\n",
    "rmse_poly = np.sqrt(np.mean( (y_test - yhat_poly)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "## [9 Points] Comparison\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** Print the RMSE for the polynomial and RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016615228635684653\n",
      "-0.0012407504525924207\n"
     ]
    }
   ],
   "source": [
    "print(acc_poly)\n",
    "print(acc_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 points]** Caluclate and print the percent improvement from the null model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-94.81389693823174\n",
      "-103.87274822523966\n"
     ]
    }
   ],
   "source": [
    "print(100*(acc_poly - acc_null)/acc_null)\n",
    "print(100*(acc_rbf - acc_null)/acc_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of these RMSE are **VERY** close to the values in HW2, however, we used a magnitude fewer observations with the SVMs!\n",
    "\n",
    "**[2 points]** Fit a linear regression using `statsmodels` on the training data (all features) used in the support vector machines in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 11 (idk if this is right tho)\n",
    "\n",
    "#### when it says all features, do we use y_train, x_train, and x_train_std? What about the x_tests?\n",
    "lin_reg = sm.OLS(y_train, sm.add_constant(x_train)).fit()\n",
    "\n",
    "#original:\n",
    "    #sm.OLS(y_train, x_train[['const', 'pct_d_rgdp', 'estabs_exit_rate']]).fit()\n",
    "\n",
    "#replace x_train with sm.add_constant xtrain \n",
    "#want to fit using specifically the statsmodel, want to pass in the xtrain to the sm model\n",
    "# sm.add_constant adds a colummn of ones and lets you predict the __ as well\n",
    "# sm.add_constant pass an xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** Print the RMSE from the OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 14\n",
    "#refit and find the accuracy with the model with the full testing data using the optimal value of `C`\n",
    "#svc_10 = LinearSVR(C = best_10['C'],\n",
    "                        #dual = False).fit(x_train_std, y_train)\n",
    "#acc_10 = svc_10.score(x_test_std, y_test)\n",
    "#acc_10\n",
    "\n",
    "########### OR #############\n",
    "# homework 2\n",
    "rmse_lin_reg = np.sqrt(np.mean((y_test - lin_reg.predict(x_test))**2))\n",
    "rmse_lin_reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "## [6 Points] A Number of Observations Tangent\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)\n",
    "\n",
    "**[2 points]** Print the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>l_incwage</td>    <th>  R-squared:         </th> <td>   0.533</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.532</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   638.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 22 Mar 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:51:05</td>     <th>  Log-Likelihood:    </th> <td> -9093.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  7858</td>      <th>  AIC:               </th> <td>1.822e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  7843</td>      <th>  BIC:               </th> <td>1.832e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>    8.6085</td> <td>    0.078</td> <td>  110.136</td> <td> 0.000</td> <td>    8.455</td> <td>    8.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>id</th>           <td>  6.28e-11</td> <td> 2.17e-10</td> <td>    0.290</td> <td> 0.772</td> <td>-3.62e-10</td> <td> 4.87e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>full_time</th>    <td>    1.1444</td> <td>    0.024</td> <td>   48.573</td> <td> 0.000</td> <td>    1.098</td> <td>    1.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>l_density</th>    <td>    0.0194</td> <td>    0.009</td> <td>    2.183</td> <td> 0.029</td> <td>    0.002</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>experience</th>   <td>    0.0170</td> <td>    0.001</td> <td>   24.792</td> <td> 0.000</td> <td>    0.016</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>puma_urate</th>   <td>   -0.0137</td> <td>    0.005</td> <td>   -2.878</td> <td> 0.004</td> <td>   -0.023</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>occ_ba</th>       <td>    0.0069</td> <td>    0.000</td> <td>   17.714</td> <td> 0.000</td> <td>    0.006</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ind_wage</th>     <td> 9.654e-06</td> <td> 4.69e-07</td> <td>   20.572</td> <td> 0.000</td> <td> 8.73e-06</td> <td> 1.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hpi</th>          <td>    0.0006</td> <td>    0.000</td> <td>    2.096</td> <td> 0.036</td> <td> 3.84e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>female</th>       <td>   -0.2704</td> <td>    0.018</td> <td>  -15.174</td> <td> 0.000</td> <td>   -0.305</td> <td>   -0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>any_kids</th>     <td>    0.1845</td> <td>    0.020</td> <td>    9.233</td> <td> 0.000</td> <td>    0.145</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>black</th>        <td>   -0.1441</td> <td>    0.029</td> <td>   -4.927</td> <td> 0.000</td> <td>   -0.201</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hispanic</th>     <td>   -0.0941</td> <td>    0.036</td> <td>   -2.596</td> <td> 0.009</td> <td>   -0.165</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hs or less</th>   <td>   -0.4805</td> <td>    0.028</td> <td>  -17.467</td> <td> 0.000</td> <td>   -0.534</td> <td>   -0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>some college</th> <td>   -0.2935</td> <td>    0.024</td> <td>  -12.054</td> <td> 0.000</td> <td>   -0.341</td> <td>   -0.246</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1814.772</td> <th>  Durbin-Watson:     </th> <td>   1.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>7813.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-1.071</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.390</td>  <th>  Cond. No.          </th> <td>7.09e+08</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.09e+08. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              l_incwage   R-squared:                       0.533\n",
       "Model:                            OLS   Adj. R-squared:                  0.532\n",
       "Method:                 Least Squares   F-statistic:                     638.2\n",
       "Date:                Mon, 22 Mar 2021   Prob (F-statistic):               0.00\n",
       "Time:                        19:51:05   Log-Likelihood:                -9093.4\n",
       "No. Observations:                7858   AIC:                         1.822e+04\n",
       "Df Residuals:                    7843   BIC:                         1.832e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const            8.6085      0.078    110.136      0.000       8.455       8.762\n",
       "id             6.28e-11   2.17e-10      0.290      0.772   -3.62e-10    4.87e-10\n",
       "full_time        1.1444      0.024     48.573      0.000       1.098       1.191\n",
       "l_density        0.0194      0.009      2.183      0.029       0.002       0.037\n",
       "experience       0.0170      0.001     24.792      0.000       0.016       0.018\n",
       "puma_urate      -0.0137      0.005     -2.878      0.004      -0.023      -0.004\n",
       "occ_ba           0.0069      0.000     17.714      0.000       0.006       0.008\n",
       "ind_wage      9.654e-06   4.69e-07     20.572      0.000    8.73e-06    1.06e-05\n",
       "hpi              0.0006      0.000      2.096      0.036    3.84e-05       0.001\n",
       "female          -0.2704      0.018    -15.174      0.000      -0.305      -0.236\n",
       "any_kids         0.1845      0.020      9.233      0.000       0.145       0.224\n",
       "black           -0.1441      0.029     -4.927      0.000      -0.201      -0.087\n",
       "hispanic        -0.0941      0.036     -2.596      0.009      -0.165      -0.023\n",
       "hs or less      -0.4805      0.028    -17.467      0.000      -0.534      -0.427\n",
       "some college    -0.2935      0.024    -12.054      0.000      -0.341      -0.246\n",
       "==============================================================================\n",
       "Omnibus:                     1814.772   Durbin-Watson:                   1.996\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7813.323\n",
       "Skew:                          -1.071   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.390   Cond. No.                     7.09e+08\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.09e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the significance levels with HW2. \n",
    "\n",
    "**[3 points]** All else equal, how does the number of observations influence your statistical significance?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "More observations allow for an increase in the statistical significance of said findings when all else is equal. A higher sample size gives us greater confidence because the larger the sample size, the more accurately it represents the behavior of all of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer back to the types of error in lecture 8.\n",
    "\n",
    "**[3 points]** What does an increase in observations increase?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As number of observations increase, the sensitivity of the hypothesis test also increases. In other words, the more observations there are means we are more likely to reject the null hypothesis when it is actually false. This implies that increasing sample size reduces the risk of making a type II error. We can also say that the confidence in our estimate increases as well because of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "## [12 Points] A Simple Problem\n",
    "[TOP](#Homework2---EDA,-Regression,-&-You)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[4 points]** Resample the data with a 50-50 train-test split using a random state of 490."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped['l_incwage']\n",
    "x = df_prepped.drop(columns = 'l_incwage')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 50/100 , random_state = 490)\n",
    "\n",
    "x_train_std = pd.DataFrame(StandardScaler().fit(x_train).transform(x_train),\n",
    "                           columns = x_train.columns,\n",
    "                           index = x_train.index)\n",
    "\n",
    "x_test_std = pd.DataFrame(StandardScaler().fit(x_test).transform(x_test),\n",
    "                          columns = x_test.columns, \n",
    "                          index = x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[4 points]** Print the RMSE of a new OLS model fitted on the newly split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 11; in comparison section above --> substitute x_train for ___.\n",
    "new_OLS_mod = sm.OLS(y_train, sm.add_constant(x_train)).fit()\n",
    "\n",
    "rmse_new_OLS_mod = np.sqrt(np.mean((y_test - new_OLS_mod.predict(x_test))**2))\n",
    "rmse_new_OLS_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points]** Based upon the $\\approx$30% improvement and nearly identical RMSE with either 50% or 0.5% sized training data, would you say that predicting wages is a relatively simple or relatively complex prediction problem?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given that there was an approximate 30% improvement and identical RMSE values between training data with sizes 0.5% and 50%, I conclude that predicting wages is a relatively simple prediction problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[for fun]** Print the summary of the second OLS model. \n",
    "Verify the p-values are truly smaller with more observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_OLS_mod.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
